extern crate alloc;

use alloc::vec::Vec;
use core::fmt;
use core::sync::atomic::{AtomicU16, Ordering};

use crate::net::packetbuf::PacketBuf;
use crate::net::tcp_socket;
use crate::net::types::{Ipv4Addr, NetError, Port, SockAddr};

/// Internal storage for protocol-specific socket state.
///
/// Phase 4A defines this enum; legacy syscall paths continue on `KernelSocket`
/// until Phase 4C/4D migration activates this framework.
pub enum SocketInner {
    /// UDP socket state (stateless at protocol level in Phase 4A).
    Udp(UdpSocketInner),
    /// TCP socket state placeholder (expanded in Phase 5).
    Tcp(TcpSocketInner),
    /// Raw socket state placeholder (expanded in Phase 9).
    Raw(RawSocketInner),
}

/// UDP protocol-specific state.
///
/// Phase 4A keeps this empty because UDP per-socket protocol state is minimal.
pub struct UdpSocketInner;

/// TCP protocol-specific state placeholder.
///
/// `conn_id` links the socket to a transport connection in future phases.
/// `listen` holds the two-queue listen state for listening sockets (Phase 5B).
pub struct TcpSocketInner {
    /// Optional transport connection identifier (Phase 5).
    pub conn_id: Option<u32>,
    /// Two-queue listen state for TCP listening sockets (Phase 5B).
    pub listen: Option<tcp_socket::TcpListenState>,
}

/// Raw socket protocol-specific state placeholder.
///
/// This remains empty in Phase 4A and is expanded in Phase 9.
pub struct RawSocketInner;

/// Socket status and mode flags.
///
/// This is a small bitflags-like wrapper with no external dependency.
#[derive(Clone, Copy, PartialEq, Eq, Debug)]
pub struct SocketFlags(u32);

impl SocketFlags {
    /// No flags set.
    pub const NONE: Self = Self(0);
    /// Non-blocking I/O mode.
    pub const O_NONBLOCK: Self = Self(1 << 0);
    /// Read side has been shut down.
    pub const SHUT_RD: Self = Self(1 << 1);
    /// Write side has been shut down.
    pub const SHUT_WR: Self = Self(1 << 2);

    /// Return `true` if all bits in `other` are set.
    pub const fn contains(self, other: Self) -> bool {
        (self.0 & other.0) == other.0
    }

    /// Set the given flag bits.
    pub fn set(&mut self, flag: Self) {
        self.0 |= flag.0;
    }

    /// Clear the given flag bits.
    pub fn clear(&mut self, flag: Self) {
        self.0 &= !flag.0;
    }

    /// Return raw bit representation.
    pub const fn bits(self) -> u32 {
        self.0
    }

    /// Construct from raw bits.
    pub const fn from_bits(bits: u32) -> Self {
        Self(bits)
    }
}

/// Per-socket configurable options.
///
/// Phase 4A defines these fields; legacy syscall paths keep their existing
/// option storage until migration phases activate this struct.
pub struct SocketOptions {
    /// Allow local address reuse.
    pub reuse_addr: bool,
    /// Receive buffer size in bytes.
    ///
    /// Default: 16384, valid range: 256..=262144.
    pub recv_buf_size: usize,
    /// Send buffer size in bytes.
    ///
    /// Default: 16384, valid range: 256..=262144.
    pub send_buf_size: usize,
    /// Receive timeout in milliseconds (`None` means infinite).
    pub recv_timeout: Option<u64>,
    /// Send timeout in milliseconds (`None` means infinite).
    pub send_timeout: Option<u64>,
    /// Enable keepalive (TCP-only behavior in later phases).
    pub keepalive: bool,
    /// Disable Nagle algorithm (TCP-only behavior in later phases).
    pub tcp_nodelay: bool,
}

impl SocketOptions {
    /// Default receive buffer size in bytes.
    pub const RECV_BUF_DEFAULT: usize = 16_384;
    /// Default send buffer size in bytes.
    pub const SEND_BUF_DEFAULT: usize = 16_384;
    /// Minimum allowed receive buffer size in bytes.
    pub const RECV_BUF_MIN: usize = 256;
    /// Maximum allowed receive buffer size in bytes.
    pub const RECV_BUF_MAX: usize = 262_144;
    /// Minimum allowed send buffer size in bytes.
    pub const SEND_BUF_MIN: usize = 256;
    /// Maximum allowed send buffer size in bytes.
    pub const SEND_BUF_MAX: usize = 262_144;

    /// Construct options with Phase 4A defaults.
    pub const fn new() -> Self {
        Self {
            reuse_addr: false,
            recv_buf_size: Self::RECV_BUF_DEFAULT,
            send_buf_size: Self::SEND_BUF_DEFAULT,
            recv_timeout: None,
            send_timeout: None,
            keepalive: false,
            tcp_nodelay: false,
        }
    }

    /// Validate and normalize a receive buffer size request.
    ///
    /// Returns `NetError::InvalidArgument` if the value is out of range.
    pub fn validate_recv_buf_size(size: usize) -> Result<usize, NetError> {
        if !(Self::RECV_BUF_MIN..=Self::RECV_BUF_MAX).contains(&size) {
            return Err(NetError::InvalidArgument);
        }
        Ok(size)
    }

    /// Validate and normalize a send buffer size request.
    ///
    /// Returns `NetError::InvalidArgument` if the value is out of range.
    pub fn validate_send_buf_size(size: usize) -> Result<usize, NetError> {
        if !(Self::SEND_BUF_MIN..=Self::SEND_BUF_MAX).contains(&size) {
            return Err(NetError::InvalidArgument);
        }
        Ok(size)
    }
}

impl Default for SocketOptions {
    fn default() -> Self {
        Self::new()
    }
}

/// Fixed-capacity queue with ring-buffer semantics.
///
/// Phase 4A defines this queue for per-socket packets. Push never overwrites:
/// it returns `false` when full.
pub struct BoundedQueue<T> {
    slots: Vec<Option<T>>,
    head: usize,
    len: usize,
}

impl<T> BoundedQueue<T> {
    /// Create a queue with `capacity` slots.
    pub fn new(capacity: usize) -> Self {
        let slots = core::iter::repeat_with(|| None).take(capacity).collect();
        Self {
            slots,
            head: 0,
            len: 0,
        }
    }

    /// Push an item to the tail.
    ///
    /// Returns `false` if the queue is full; no item is overwritten.
    pub fn push(&mut self, item: T) -> bool {
        if self.is_full() {
            return false;
        }
        let cap = self.capacity();
        if cap == 0 {
            return false;
        }
        let tail = (self.head + self.len) % cap;
        self.slots[tail] = Some(item);
        self.len += 1;
        true
    }

    /// Pop an item from the head.
    pub fn pop(&mut self) -> Option<T> {
        if self.is_empty() {
            return None;
        }
        let cap = self.capacity();
        if cap == 0 {
            return None;
        }
        let idx = self.head;
        self.head = (self.head + 1) % cap;
        self.len -= 1;
        self.slots[idx].take()
    }

    /// Return `true` if the queue has no elements.
    pub const fn is_empty(&self) -> bool {
        self.len == 0
    }

    /// Return `true` if the queue cannot accept more elements.
    pub fn is_full(&self) -> bool {
        self.len == self.capacity()
    }

    /// Number of queued items.
    pub const fn len(&self) -> usize {
        self.len
    }

    /// Maximum number of storable items.
    pub fn capacity(&self) -> usize {
        self.slots.len()
    }

    /// Clear all queued items.
    pub fn clear(&mut self) {
        for slot in &mut self.slots {
            let _ = slot.take();
        }
        self.head = 0;
        self.len = 0;
    }

    /// Resize queue capacity, preserving item order.
    ///
    /// If `new_capacity` is smaller than current length, oldest items are kept
    /// until capacity is reached and the rest are dropped.
    pub fn resize(&mut self, new_capacity: usize) {
        let mut drained = Vec::with_capacity(self.len);
        while let Some(item) = self.pop() {
            drained.push(item);
        }

        self.slots = core::iter::repeat_with(|| None)
            .take(new_capacity)
            .collect::<Vec<Option<T>>>();
        self.head = 0;
        self.len = 0;

        for item in drained {
            if !self.push(item) {
                break;
            }
        }
    }
}

impl<T> fmt::Debug for BoundedQueue<T> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("BoundedQueue")
            .field("len", &self.len)
            .field("capacity", &self.capacity())
            .finish()
    }
}

/// Next wait-queue hint used by `Socket::new` placeholders.
///
/// Phase 6 replaces these indices with real queue registrations.
static SOCKET_WQ_HINT: AtomicU16 = AtomicU16::new(0);

/// Unified socket object for the new framework.
///
/// Phase 4A defines this object. Legacy code paths continue using
/// `KernelSocket` until Phase 4C/4D migration.
pub struct Socket {
    /// Protocol-specific socket state.
    pub inner: SocketInner,
    /// Generic lifecycle state.
    pub state: SocketState,
    /// Mode/shutdown flags.
    pub flags: SocketFlags,
    /// Socket options.
    pub options: SocketOptions,
    /// Optional bound local address.
    pub local_addr: Option<SockAddr>,
    /// Optional connected peer address.
    pub remote_addr: Option<SockAddr>,
    /// Receive queue of `(packet, source address)` tuples.
    pub recv_queue: BoundedQueue<(PacketBuf, SockAddr)>,
    /// Deferred error reported on next operation.
    pub pending_error: Option<NetError>,
    /// Owning process identifier.
    pub process_id: u32,
    /// Placeholder receive wait queue index (Phase 6 replacement planned).
    pub recv_wq_idx: u8,
    /// Placeholder accept wait queue index (Phase 6 replacement planned).
    pub accept_wq_idx: u8,
    /// Placeholder send wait queue index (Phase 6 replacement planned).
    pub send_wq_idx: u8,
}

impl Socket {
    /// Default receive queue capacity in packets.
    pub const RECV_QUEUE_DEFAULT_CAPACITY: usize = 16;

    /// Create a new socket object with Phase 4A defaults.
    pub fn new(inner: SocketInner) -> Self {
        let wq_idx = (SOCKET_WQ_HINT.fetch_add(1, Ordering::Relaxed) & 0x00FF) as u8;
        Self {
            inner,
            state: SocketState::Unbound,
            flags: SocketFlags::NONE,
            options: SocketOptions::new(),
            local_addr: None,
            remote_addr: None,
            recv_queue: BoundedQueue::new(Self::RECV_QUEUE_DEFAULT_CAPACITY),
            pending_error: None,
            process_id: 0,
            recv_wq_idx: wq_idx,
            accept_wq_idx: wq_idx,
            send_wq_idx: wq_idx,
        }
    }

    /// Return `true` if non-blocking mode is enabled.
    pub fn is_nonblocking(&self) -> bool {
        self.flags.contains(SocketFlags::O_NONBLOCK)
    }

    /// Return `true` if read shutdown is active.
    pub fn is_read_shutdown(&self) -> bool {
        self.flags.contains(SocketFlags::SHUT_RD)
    }

    /// Return `true` if write shutdown is active.
    pub fn is_write_shutdown(&self) -> bool {
        self.flags.contains(SocketFlags::SHUT_WR)
    }

    /// Enable or disable non-blocking mode.
    pub fn set_nonblocking(&mut self, nonblocking: bool) {
        if nonblocking {
            self.flags.set(SocketFlags::O_NONBLOCK);
        } else {
            self.flags.clear(SocketFlags::O_NONBLOCK);
        }
    }

    /// Take and clear any pending error.
    pub fn take_pending_error(&mut self) -> Option<NetError> {
        self.pending_error.take()
    }
}

/// Slab-like socket table with freelist allocation.
///
/// Phase 4A defines this table. Syscall handlers still use the legacy table
/// until Phase 4C/4D migration.
pub struct SlabSocketTable {
    slots: Vec<Option<Socket>>,
    freelist: Vec<usize>,
    max_capacity: usize,
}

impl SlabSocketTable {
    /// Default initial slot count.
    pub const INITIAL_CAPACITY: usize = 64;
    /// Hard maximum slot count.
    pub const MAX_CAPACITY: usize = 1024;

    /// Create an empty, const-initializable table.
    ///
    /// This is used for global static initialization; first use should call
    /// [`init_if_needed`](Self::init_if_needed).
    pub const fn empty() -> Self {
        Self {
            slots: Vec::new(),
            freelist: Vec::new(),
            max_capacity: 0,
        }
    }

    /// Lazily initialize with default capacities if currently empty.
    pub fn init_if_needed(&mut self) {
        if self.max_capacity == 0 {
            *self = Self::new(Self::INITIAL_CAPACITY, Self::MAX_CAPACITY);
        }
    }

    /// Create a slab table with explicit initial and maximum capacities.
    ///
    /// Freelist is populated in reverse so index 0 is allocated first.
    pub fn new(initial_capacity: usize, max_capacity: usize) -> Self {
        let init_cap = core::cmp::min(initial_capacity, max_capacity);
        let mut slots = core::iter::repeat_with(|| None)
            .take(init_cap)
            .collect::<Vec<Option<Socket>>>();
        if slots.len() != init_cap {
            slots.clear();
        }
        let freelist = (0..init_cap).rev().collect();
        Self {
            slots,
            freelist,
            max_capacity,
        }
    }

    /// Allocate a new socket slot.
    ///
    /// Returns the socket index on success. If no free slots are available,
    /// attempts to grow capacity (doubling, capped at `max_capacity`).
    pub fn alloc(&mut self, inner: SocketInner) -> Option<usize> {
        self.init_if_needed();
        if self.freelist.is_empty() {
            self.grow();
        }
        let idx = self.freelist.pop()?;
        self.slots[idx] = Some(Socket::new(inner));
        Some(idx)
    }

    /// Get an immutable socket reference by index.
    pub fn get(&self, idx: usize) -> Option<&Socket> {
        self.slots.get(idx)?.as_ref()
    }

    /// Get a mutable socket reference by index.
    pub fn get_mut(&mut self, idx: usize) -> Option<&mut Socket> {
        self.slots.get_mut(idx)?.as_mut()
    }

    /// Free an active slot and return it to the freelist.
    pub fn free(&mut self, idx: usize) {
        if let Some(slot) = self.slots.get_mut(idx) {
            if slot.take().is_some() {
                self.freelist.push(idx);
            }
        }
    }

    /// Number of active sockets.
    pub fn count_active(&self) -> usize {
        self.slots.iter().filter(|s| s.is_some()).count()
    }

    /// Current slot capacity.
    pub fn capacity(&self) -> usize {
        self.slots.len()
    }

    /// Number of active sockets (alias of [`count_active`](Self::count_active)).
    pub fn len(&self) -> usize {
        self.count_active()
    }

    fn grow(&mut self) {
        let current = self.slots.len();
        if current >= self.max_capacity {
            return;
        }

        let mut new_cap = if current == 0 {
            Self::INITIAL_CAPACITY
        } else {
            current.saturating_mul(2)
        };
        if new_cap > self.max_capacity {
            new_cap = self.max_capacity;
        }
        if new_cap <= current {
            return;
        }

        let add = new_cap - current;
        self.slots.extend(
            core::iter::repeat_with(|| None)
                .take(add)
                .collect::<Vec<_>>(),
        );
        for idx in (current..new_cap).rev() {
            self.freelist.push(idx);
        }
    }
}

/// Ephemeral port allocator for dynamic local port selection.
///
/// Phase 4A defines this allocator and both old/new socket paths may use it.
/// Access must be serialized by the outer lock (no internal atomics).
pub struct EphemeralPortAllocator {
    bitmap: [u8; Self::BITMAP_SIZE],
    next_port: u16,
    allocated_count: usize,
}

impl EphemeralPortAllocator {
    /// Start of IANA ephemeral range.
    pub const EPHEMERAL_PORT_START: u16 = 49_152;
    /// End of IANA ephemeral range.
    pub const EPHEMERAL_PORT_END: u16 = 65_535;
    /// Total number of ephemeral ports.
    pub const EPHEMERAL_PORT_COUNT: usize = 16_384;
    /// Bitmap size in bytes (`EPHEMERAL_PORT_COUNT / 8`).
    pub const BITMAP_SIZE: usize = 2048;

    /// Create a fresh allocator with no allocated ports.
    pub const fn new() -> Self {
        Self {
            bitmap: [0; Self::BITMAP_SIZE],
            next_port: Self::EPHEMERAL_PORT_START,
            allocated_count: 0,
        }
    }

    /// Allocate one ephemeral port using round-robin selection.
    ///
    /// Returns `None` if all ephemeral ports are currently allocated.
    pub fn alloc(&mut self) -> Option<Port> {
        if self.allocated_count >= Self::EPHEMERAL_PORT_COUNT {
            return None;
        }

        let start = self.next_port;
        for offset in 0..Self::EPHEMERAL_PORT_COUNT {
            let candidate = Self::EPHEMERAL_PORT_START
                + ((start - Self::EPHEMERAL_PORT_START + offset as u16)
                    % Self::EPHEMERAL_PORT_COUNT as u16);
            if !self.is_allocated(candidate) {
                self.set_allocated(candidate);
                self.allocated_count += 1;
                self.next_port = if candidate == Self::EPHEMERAL_PORT_END {
                    Self::EPHEMERAL_PORT_START
                } else {
                    candidate + 1
                };
                return Some(Port(candidate));
            }
        }

        None
    }

    /// Release a previously allocated ephemeral port.
    pub fn release(&mut self, port: Port) {
        let p = port.0;
        if !(Self::EPHEMERAL_PORT_START..=Self::EPHEMERAL_PORT_END).contains(&p) {
            return;
        }
        if self.is_allocated(p) {
            self.clear_allocated(p);
            self.allocated_count -= 1;
        }
    }

    /// Return `true` if `port` is currently allocated.
    pub fn is_in_use(&self, port: Port) -> bool {
        let p = port.0;
        if !(Self::EPHEMERAL_PORT_START..=Self::EPHEMERAL_PORT_END).contains(&p) {
            return false;
        }
        self.is_allocated(p)
    }

    /// Number of currently available ephemeral ports.
    pub fn available(&self) -> usize {
        Self::EPHEMERAL_PORT_COUNT - self.allocated_count
    }

    fn port_to_bit_index(port: u16) -> usize {
        (port - Self::EPHEMERAL_PORT_START) as usize
    }

    fn is_allocated(&self, port: u16) -> bool {
        let bit = Self::port_to_bit_index(port);
        let byte = bit / 8;
        let mask = 1u8 << (bit % 8);
        (self.bitmap[byte] & mask) != 0
    }

    fn set_allocated(&mut self, port: u16) {
        let bit = Self::port_to_bit_index(port);
        let byte = bit / 8;
        let mask = 1u8 << (bit % 8);
        self.bitmap[byte] |= mask;
    }

    fn clear_allocated(&mut self, port: u16) {
        let bit = Self::port_to_bit_index(port);
        let byte = bit / 8;
        let mask = 1u8 << (bit % 8);
        self.bitmap[byte] &= !mask;
    }
}

impl Default for EphemeralPortAllocator {
    fn default() -> Self {
        Self::new()
    }
}

/// New slab-based socket table (Phase 4A).
///
/// Initially unused by legacy socket syscalls. Migration occurs in Phase 4C/4D.
pub static NEW_SOCKET_TABLE: slopos_lib::IrqMutex<SlabSocketTable> =
    slopos_lib::IrqMutex::new(SlabSocketTable::empty());

/// Ephemeral port allocator (Phase 4A).
///
/// Shared infrastructure for both legacy and future socket paths.
pub static EPHEMERAL_PORTS: slopos_lib::IrqMutex<EphemeralPortAllocator> =
    slopos_lib::IrqMutex::new(EphemeralPortAllocator::new());

// =============================================================================
// LEGACY: Existing socket infrastructure (pre-Phase 4A)
//
// The code below implements the current socket functionality using KernelSocket
// and a fixed-size SocketTable. Phase 4C/4D will migrate this to use the new
// Socket framework above. Until then, both coexist.
// =============================================================================

use core::cmp;

use slopos_abi::net::{AF_INET, MAX_SOCKETS, SOCK_DGRAM, SOCK_STREAM};
use slopos_abi::syscall::{
    ERRNO_EADDRINUSE, ERRNO_EAFNOSUPPORT, ERRNO_EAGAIN, ERRNO_ECONNREFUSED, ERRNO_EDESTADDRREQ,
    ERRNO_EFAULT, ERRNO_EINVAL, ERRNO_EISCONN, ERRNO_ENETUNREACH, ERRNO_ENOMEM, ERRNO_ENOTCONN,
    ERRNO_ENOTSOCK, ERRNO_EPIPE, ERRNO_EPROTONOSUPPORT, POLLERR, POLLHUP, POLLIN, POLLOUT,
};
use slopos_lib::{IrqMutex, WaitQueue};

use crate::net;
use crate::net::tcp::{self, TCP_HEADER_LEN, TcpError, TcpOutSegment, TcpState};
use crate::virtio_net;

const TCP_TX_MAX: usize = 1460;
pub const UDP_DGRAM_MAX_PAYLOAD: usize = 1472;
pub const UDP_RX_QUEUE_SIZE: usize = 16;

#[derive(Clone, Copy, PartialEq, Eq, Debug)]
pub enum SocketState {
    Unbound,
    Bound,
    Listening,
    Connecting,
    Connected,
    Closed,
}

#[derive(Clone, Copy)]
pub struct UdpDatagram {
    pub src_ip: [u8; 4],
    pub src_port: u16,
    pub len: u16,
    pub data: [u8; UDP_DGRAM_MAX_PAYLOAD],
}

impl UdpDatagram {
    pub const fn empty() -> Self {
        Self {
            src_ip: [0; 4],
            src_port: 0,
            len: 0,
            data: [0; UDP_DGRAM_MAX_PAYLOAD],
        }
    }
}

pub struct UdpReceiveQueue {
    slots: [UdpDatagram; UDP_RX_QUEUE_SIZE],
    head: usize,
    len: usize,
}

impl UdpReceiveQueue {
    pub const fn new() -> Self {
        Self {
            slots: [UdpDatagram::empty(); UDP_RX_QUEUE_SIZE],
            head: 0,
            len: 0,
        }
    }

    pub fn push(&mut self, dgram: &UdpDatagram) {
        if self.len == UDP_RX_QUEUE_SIZE {
            self.slots[self.head] = *dgram;
            self.head = (self.head + 1) % UDP_RX_QUEUE_SIZE;
            return;
        }

        let tail = (self.head + self.len) % UDP_RX_QUEUE_SIZE;
        self.slots[tail] = *dgram;
        self.len += 1;
    }

    pub fn pop(&mut self) -> Option<UdpDatagram> {
        if self.len == 0 {
            return None;
        }

        let dgram = self.slots[self.head];
        self.head = (self.head + 1) % UDP_RX_QUEUE_SIZE;
        self.len -= 1;
        Some(dgram)
    }

    pub fn is_empty(&self) -> bool {
        self.len == 0
    }

    pub fn len(&self) -> usize {
        self.len
    }

    pub fn clear(&mut self) {
        self.head = 0;
        self.len = 0;
    }
}

static RECV_WQS: [WaitQueue; MAX_SOCKETS] = {
    const WAIT_QUEUE: WaitQueue = WaitQueue::new();
    [WAIT_QUEUE; MAX_SOCKETS]
};
static ACCEPT_WQS: [WaitQueue; MAX_SOCKETS] = {
    const WAIT_QUEUE: WaitQueue = WaitQueue::new();
    [WAIT_QUEUE; MAX_SOCKETS]
};
static SEND_WQS: [WaitQueue; MAX_SOCKETS] = {
    const WAIT_QUEUE: WaitQueue = WaitQueue::new();
    [WAIT_QUEUE; MAX_SOCKETS]
};
pub static UDP_RX_QUEUES: [IrqMutex<UdpReceiveQueue>; MAX_SOCKETS] =
    [const { IrqMutex::new(UdpReceiveQueue::new()) }; MAX_SOCKETS];

fn errno_i32(errno: u64) -> i32 {
    errno as i64 as i32
}

fn map_tcp_err(err: TcpError) -> i32 {
    match err {
        TcpError::NotFound => errno_i32(ERRNO_ENOTSOCK),
        TcpError::InvalidState => errno_i32(ERRNO_ENOTCONN),
        TcpError::AddrInUse => errno_i32(ERRNO_EADDRINUSE),
        TcpError::TableFull => errno_i32(ERRNO_ENOMEM),
        TcpError::ConnectionRefused => errno_i32(ERRNO_ECONNREFUSED),
        TcpError::ConnectionReset => errno_i32(ERRNO_ECONNREFUSED),
        TcpError::TimedOut => errno_i32(ERRNO_EAGAIN),
        TcpError::InvalidSegment => errno_i32(ERRNO_EINVAL),
    }
}

fn map_tcp_err_i64(err: TcpError) -> i64 {
    map_tcp_err(err) as i64
}

fn map_net_err(err: NetError) -> i32 {
    match err {
        NetError::AddressInUse => errno_i32(ERRNO_EADDRINUSE),
        NetError::AddressFamilyNotSupported => errno_i32(ERRNO_EAFNOSUPPORT),
        NetError::WouldBlock => errno_i32(ERRNO_EAGAIN),
        NetError::NotConnected => errno_i32(ERRNO_ENOTCONN),
        NetError::AlreadyConnected => errno_i32(ERRNO_EISCONN),
        NetError::ProtocolNotSupported => errno_i32(ERRNO_EPROTONOSUPPORT),
        NetError::NetworkUnreachable | NetError::HostUnreachable => errno_i32(ERRNO_ENETUNREACH),
        NetError::NoBufferSpace => errno_i32(ERRNO_ENOMEM),
        NetError::Shutdown => errno_i32(ERRNO_EPIPE),
        _ => errno_i32(ERRNO_EINVAL),
    }
}

fn alloc_ephemeral_port() -> Option<Port> {
    EPHEMERAL_PORTS.lock().alloc()
}

fn be_port(port: u16) -> [u8; 2] {
    port.to_be_bytes()
}

fn write_tcp_segment(seg: &TcpOutSegment, payload: &[u8], out: &mut [u8]) -> Option<usize> {
    let opt_len = if seg.mss != 0 { 4usize } else { 0usize };
    let data_offset_words = ((TCP_HEADER_LEN + opt_len) / 4) as u8;
    let tcp_len = TCP_HEADER_LEN + opt_len + payload.len();
    if out.len() < tcp_len {
        return None;
    }

    let hdr = tcp::build_header(
        seg.tuple.local_port,
        seg.tuple.remote_port,
        seg.seq_num,
        seg.ack_num,
        seg.flags,
        seg.window_size,
        data_offset_words,
    );
    let hdr_len = tcp::write_header(&hdr, out)?;

    if seg.mss != 0 {
        let opt_start = TCP_HEADER_LEN;
        out[opt_start] = tcp::TCP_OPT_MSS;
        out[opt_start + 1] = tcp::TCP_OPT_MSS_LEN;
        out[opt_start + 2..opt_start + 4].copy_from_slice(&seg.mss.to_be_bytes());
    }

    out[hdr_len..hdr_len + payload.len()].copy_from_slice(payload);

    let checksum = tcp::tcp_checksum(seg.tuple.local_ip, seg.tuple.remote_ip, &out[..tcp_len]);
    out[16..18].copy_from_slice(&checksum.to_be_bytes());
    Some(tcp_len)
}

pub(crate) fn socket_send_tcp_segment(seg: &TcpOutSegment, payload: &[u8]) -> i32 {
    let src_mac = virtio_net::virtio_net_mac().unwrap_or([0; 6]);
    let dst_mac = [0xff; 6];

    let ip_total_len =
        net::IPV4_HEADER_LEN + TCP_HEADER_LEN + if seg.mss != 0 { 4 } else { 0 } + payload.len();
    let frame_len = net::ETH_HEADER_LEN + ip_total_len;
    let mut frame = [0u8; 1600];
    if frame_len > frame.len() {
        return errno_i32(ERRNO_EINVAL);
    }

    frame[0..6].copy_from_slice(&dst_mac);
    frame[6..12].copy_from_slice(&src_mac);
    frame[12..14].copy_from_slice(&net::ETHERTYPE_IPV4.to_be_bytes());

    let ip = net::ETH_HEADER_LEN;
    frame[ip] = 0x45;
    frame[ip + 1] = 0;
    frame[ip + 2..ip + 4].copy_from_slice(&(ip_total_len as u16).to_be_bytes());
    frame[ip + 4..ip + 6].copy_from_slice(&0u16.to_be_bytes());
    frame[ip + 6..ip + 8].copy_from_slice(&0u16.to_be_bytes());
    frame[ip + 8] = 64;
    frame[ip + 9] = net::IPPROTO_TCP;
    frame[ip + 10..ip + 12].copy_from_slice(&0u16.to_be_bytes());
    frame[ip + 12..ip + 16].copy_from_slice(&seg.tuple.local_ip);
    frame[ip + 16..ip + 20].copy_from_slice(&seg.tuple.remote_ip);
    let ip_csum = net::ipv4_header_checksum(&frame[ip..ip + net::IPV4_HEADER_LEN]);
    frame[ip + 10..ip + 12].copy_from_slice(&ip_csum.to_be_bytes());

    let tcp_start = ip + net::IPV4_HEADER_LEN;
    let tcp_len = match write_tcp_segment(seg, payload, &mut frame[tcp_start..]) {
        Some(n) => n,
        None => return errno_i32(ERRNO_EINVAL),
    };

    let total = net::ETH_HEADER_LEN + net::IPV4_HEADER_LEN + tcp_len;
    if !virtio_net::virtio_net_is_ready() {
        return 0;
    }

    if virtio_net::virtio_net_transmit(&frame[..total]) {
        0
    } else {
        0
    }
}

fn wq_slot(hint: u8) -> usize {
    (hint as usize) % MAX_SOCKETS
}

fn socket_wake_recv_hint(wq_hint: u8) {
    RECV_WQS[wq_slot(wq_hint)].wake_all();
}

fn socket_wake_send_hint(wq_hint: u8) {
    SEND_WQS[wq_slot(wq_hint)].wake_all();
}

fn socket_wake_accept_hint(wq_hint: u8) {
    ACCEPT_WQS[wq_slot(wq_hint)].wake_all();
}

fn socket_tcp_conn_id(sock: &Socket) -> Option<usize> {
    match &sock.inner {
        SocketInner::Tcp(tcp) => tcp.conn_id.map(|id| id as usize),
        _ => None,
    }
}

fn socket_is_udp(sock: &Socket) -> bool {
    matches!(sock.inner, SocketInner::Udp(_))
}

fn socket_notify_tcp_idx_waiters(tcp_idx: usize) {
    let table = NEW_SOCKET_TABLE.lock();
    for slot in table.slots.iter().flatten() {
        if socket_tcp_conn_id(slot) != Some(tcp_idx) {
            continue;
        }
        if tcp::tcp_recv_available(tcp_idx) > 0 || tcp::tcp_is_peer_closed(tcp_idx) {
            socket_wake_recv_hint(slot.recv_wq_idx);
        }
        if tcp::tcp_send_buffer_space(tcp_idx) > 0 {
            socket_wake_send_hint(slot.send_wq_idx);
        }
        if !matches!(
            tcp::tcp_get_state(tcp_idx),
            Some(TcpState::Established | TcpState::CloseWait)
        ) {
            socket_wake_recv_hint(slot.recv_wq_idx);
            socket_wake_send_hint(slot.send_wq_idx);
        }
    }
}

fn socket_notify_accept_waiters() {
    let mut table = NEW_SOCKET_TABLE.lock();
    for sock in table.slots.iter_mut().flatten() {
        if sock.state != SocketState::Listening {
            continue;
        }
        // Phase 5C: Check accept queue in TcpListenState.
        let has_pending = if let SocketInner::Tcp(ref tcp_inner) = sock.inner {
            tcp_inner
                .listen
                .as_ref()
                .map(|ls| ls.accept_queue_len() > 0)
                .unwrap_or(false)
        } else {
            false
        };
        if has_pending {
            socket_wake_accept_hint(sock.accept_wq_idx);
        }
    }
}

pub fn socket_notify_tcp_activity(result: &tcp::TcpInputResult) {
    if let Some(tcp_idx) = result.conn_idx {
        socket_notify_tcp_idx_waiters(tcp_idx);

        // Phase 5B: When a connection transitions to Established, register it
        // in the TCP demux table for fast 4-tuple lookup.
        if result.new_state == Some(TcpState::Established) {
            if let Some(conn) = tcp::tcp_get_connection(tcp_idx) {
                let _ = tcp_socket::TCP_DEMUX.lock().register_established(
                    Ipv4Addr(conn.tuple.local_ip),
                    Port(conn.tuple.local_port),
                    Ipv4Addr(conn.tuple.remote_ip),
                    Port(conn.tuple.remote_port),
                    tcp_idx as u32,
                );

                // Phase 5C: Wire completed 3WHS into the listener's accept queue.
                // When a server-side child connection transitions SYN_RECEIVED -> Established,
                // find the parent listener and push an AcceptedConn to its accept queue.
                let listener_sock_idx = tcp_socket::TCP_DEMUX
                    .lock()
                    .lookup_listener(Ipv4Addr(conn.tuple.local_ip), Port(conn.tuple.local_port));
                if let Some(listener_idx) = listener_sock_idx {
                    let mut table = NEW_SOCKET_TABLE.lock();
                    if let Some(listener_sock) = table.get_mut(listener_idx as usize)
                        && listener_sock.state == SocketState::Listening
                        && let SocketInner::Tcp(ref mut tcp_inner) = listener_sock.inner
                        && let Some(ref mut listen_state) = tcp_inner.listen
                    {
                        let accepted = tcp_socket::AcceptedConn {
                            tuple: conn.tuple,
                            iss: conn.iss,
                            irs: conn.irs,
                            peer_mss: conn.peer_mss,
                        };
                        listen_state.push_accepted(accepted);
                    }
                }
            }
        }
    }
    if result.accepted_idx.is_some() || result.new_state == Some(TcpState::Established) {
        socket_notify_accept_waiters();
    }
}

fn sync_socket_state(sock: &mut Socket) {
    if let Some(tcp_idx) = socket_tcp_conn_id(sock)
        && let Some(state) = tcp::tcp_get_state(tcp_idx)
    {
        if state == TcpState::Established {
            sock.state = SocketState::Connected;
        }
        if matches!(
            state,
            TcpState::Closed
                | TcpState::TimeWait
                | TcpState::Closing
                | TcpState::LastAck
                | TcpState::FinWait1
                | TcpState::FinWait2
        ) {
            sock.state = SocketState::Closed;
        }
    }
}

pub fn socket_deliver_udp(sock_idx: u32, src_ip: [u8; 4], src_port: u16, payload: &[u8]) {
    let packet = match PacketBuf::from_raw_copy(payload) {
        Some(pkt) => pkt,
        None => return,
    };

    let mut wake_hint = None;
    {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return;
        };
        if !socket_is_udp(sock) {
            return;
        }
        let src = SockAddr::new(Ipv4Addr(src_ip), Port(src_port));
        if sock.recv_queue.push((packet, src)) {
            wake_hint = Some(sock.recv_wq_idx);
        }
    }

    if let Some(hint) = wake_hint {
        socket_wake_recv_hint(hint);
    }
}

pub fn socket_deliver_udp_from_dispatch(
    src_ip: [u8; 4],
    dst_ip: [u8; 4],
    src_port: u16,
    dst_port: u16,
    payload: &[u8],
) {
    let mut exact = None;
    let mut wildcard = None;
    {
        let table = NEW_SOCKET_TABLE.lock();
        for (idx, sock) in table.slots.iter().enumerate() {
            let Some(sock) = sock else {
                continue;
            };
            if !socket_is_udp(sock) {
                continue;
            }
            if !matches!(sock.state, SocketState::Bound | SocketState::Connected) {
                continue;
            }
            let Some(local) = sock.local_addr else {
                continue;
            };
            if local.port.0 != dst_port {
                continue;
            }

            if local.ip.0 == dst_ip {
                exact = Some(idx as u32);
                break;
            }
            if local.ip == Ipv4Addr::UNSPECIFIED {
                wildcard = Some(idx as u32);
            }
        }
    }

    if let Some(sock_idx) = exact.or(wildcard) {
        socket_deliver_udp(sock_idx, src_ip, src_port, payload);
    }
}

pub fn socket_create(domain: u16, sock_type: u16, _protocol: u16) -> i32 {
    if domain != AF_INET {
        return errno_i32(ERRNO_EAFNOSUPPORT);
    }

    let inner = match sock_type {
        SOCK_DGRAM => SocketInner::Udp(UdpSocketInner),
        SOCK_STREAM => SocketInner::Tcp(TcpSocketInner {
            conn_id: None,
            listen: None,
        }),
        _ => return errno_i32(ERRNO_EPROTONOSUPPORT),
    };

    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(idx) = table.alloc(inner) else {
        return errno_i32(ERRNO_ENOMEM);
    };
    if let Some(sock) = table.get_mut(idx) {
        sock.recv_queue.clear();
        sock.set_nonblocking(true);
    }
    idx as i32
}

pub fn socket_sendto(
    sock_idx: u32,
    data: *const u8,
    len: usize,
    dst_ip: [u8; 4],
    dst_port: u16,
) -> i64 {
    if data.is_null() && len != 0 {
        return errno_i32(ERRNO_EFAULT) as i64;
    }
    if dst_port == 0 {
        return errno_i32(ERRNO_EDESTADDRREQ) as i64;
    }
    if len > UDP_DGRAM_MAX_PAYLOAD {
        return errno_i32(ERRNO_EINVAL) as i64;
    }

    let mut auto_bind: Option<(SockAddr, bool)> = None;
    let local = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        if !socket_is_udp(sock) {
            return errno_i32(ERRNO_EPROTONOSUPPORT) as i64;
        }
        if sock.is_write_shutdown() {
            return errno_i32(ERRNO_EPIPE) as i64;
        }

        if sock.local_addr.is_none() || sock.local_addr.map(|a| a.port.0 == 0).unwrap_or(true) {
            let Some(port) = alloc_ephemeral_port() else {
                return errno_i32(ERRNO_ENOMEM) as i64;
            };
            let local_ip = crate::net::netstack::NET_STACK
                .first_ipv4()
                .map(|ip| ip.0)
                .unwrap_or([0; 4]);
            let bind_addr = SockAddr::new(Ipv4Addr(local_ip), port);
            sock.local_addr = Some(bind_addr);
            if sock.state == SocketState::Unbound {
                sock.state = SocketState::Bound;
            }
            auto_bind = Some((bind_addr, sock.options.reuse_addr));
            bind_addr
        } else {
            sock.local_addr.unwrap()
        }
    };

    if let Some((bind_addr, reuse_addr)) = auto_bind
        && let Err(err) =
            crate::net::udp::udp_bind(sock_idx, bind_addr.ip, bind_addr.port, reuse_addr)
    {
        let mut table = NEW_SOCKET_TABLE.lock();
        if let Some(sock) = table.get_mut(sock_idx as usize)
            && socket_is_udp(sock)
            && sock.local_addr == Some(bind_addr)
            && sock.state == SocketState::Bound
        {
            sock.local_addr = None;
            sock.state = SocketState::Unbound;
        }
        EPHEMERAL_PORTS.lock().release(bind_addr.port);
        return map_net_err(err) as i64;
    }

    let payload = if len == 0 {
        &[][..]
    } else {
        unsafe { core::slice::from_raw_parts(data, len) }
    };

    match crate::net::udp::udp_sendto(local.ip.0, dst_ip, local.port.0, dst_port, payload) {
        Ok(n) => n as i64,
        Err(err) => map_net_err(err) as i64,
    }
}

pub fn socket_recvfrom(
    sock_idx: u32,
    buf: *mut u8,
    len: usize,
    src_ip: *mut [u8; 4],
    src_port: *mut u16,
) -> i64 {
    if buf.is_null() && len != 0 {
        return errno_i32(ERRNO_EFAULT) as i64;
    }

    let out = if len == 0 {
        &mut [][..]
    } else {
        unsafe { core::slice::from_raw_parts_mut(buf, len) }
    };

    let (nonblocking, timeout_ms, recv_hint) = {
        let table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        if !socket_is_udp(sock) {
            return errno_i32(ERRNO_EPROTONOSUPPORT) as i64;
        }
        if sock.is_read_shutdown() {
            return 0;
        }
        (
            sock.is_nonblocking(),
            sock.options.recv_timeout.unwrap_or(0),
            sock.recv_wq_idx,
        )
    };

    loop {
        let packet = {
            let mut table = NEW_SOCKET_TABLE.lock();
            let Some(sock) = table.get_mut(sock_idx as usize) else {
                return errno_i32(ERRNO_ENOTSOCK) as i64;
            };
            sock.recv_queue.pop()
        };

        if let Some((pkt, src)) = packet {
            let payload = pkt.payload();
            let copy_len = cmp::min(out.len(), payload.len());
            out[..copy_len].copy_from_slice(&payload[..copy_len]);

            if !src_ip.is_null() {
                unsafe {
                    *src_ip = src.ip.0;
                }
            }
            if !src_port.is_null() {
                unsafe {
                    *src_port = src.port.0;
                }
            }
            return copy_len as i64;
        }

        if nonblocking {
            return errno_i32(ERRNO_EAGAIN) as i64;
        }

        let wait_ok = if timeout_ms > 0 {
            RECV_WQS[wq_slot(recv_hint)].wait_event_timeout(
                || {
                    let table = NEW_SOCKET_TABLE.lock();
                    table
                        .get(sock_idx as usize)
                        .map(|sock| !sock.recv_queue.is_empty())
                        .unwrap_or(true)
                },
                timeout_ms,
            )
        } else {
            RECV_WQS[wq_slot(recv_hint)].wait_event(|| {
                let table = NEW_SOCKET_TABLE.lock();
                table
                    .get(sock_idx as usize)
                    .map(|sock| !sock.recv_queue.is_empty())
                    .unwrap_or(true)
            })
        };

        if !wait_ok {
            return errno_i32(ERRNO_EAGAIN) as i64;
        }
    }
}

pub fn socket_bind(sock_idx: u32, addr: [u8; 4], port: u16) -> i32 {
    let mut udp_bind_args: Option<(SockAddr, bool)> = None;
    {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK);
        };

        if sock.state != SocketState::Unbound {
            return errno_i32(ERRNO_EINVAL);
        }

        let local = SockAddr::new(Ipv4Addr(addr), Port(port));
        sock.local_addr = Some(local);
        sock.state = SocketState::Bound;

        if socket_is_udp(sock) {
            udp_bind_args = Some((local, sock.options.reuse_addr));
        }
    }

    if let Some((local, reuse_addr)) = udp_bind_args
        && let Err(err) = crate::net::udp::udp_bind(sock_idx, local.ip, local.port, reuse_addr)
    {
        let mut table = NEW_SOCKET_TABLE.lock();
        if let Some(sock) = table.get_mut(sock_idx as usize)
            && socket_is_udp(sock)
            && sock.local_addr == Some(local)
            && sock.state == SocketState::Bound
        {
            sock.local_addr = None;
            sock.state = SocketState::Unbound;
        }
        return map_net_err(err);
    }

    0
}

pub fn socket_listen(sock_idx: u32, backlog: u32) -> i32 {
    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };

    let local = match sock.local_addr {
        Some(addr) => addr,
        None => return errno_i32(ERRNO_EINVAL),
    };
    if !matches!(sock.inner, SocketInner::Tcp(_)) {
        return errno_i32(ERRNO_EPROTONOSUPPORT);
    }
    if sock.state != SocketState::Bound {
        return errno_i32(ERRNO_EINVAL);
    }

    match tcp::tcp_listen(local.ip.0, local.port.0) {
        Ok(tcp_idx) => {
            if let SocketInner::Tcp(tcp_inner) = &mut sock.inner {
                tcp_inner.conn_id = Some(tcp_idx as u32);
                // Phase 5C: Create TcpListenState with two-queue model.
                tcp_inner.listen = Some(tcp_socket::TcpListenState::new(backlog as usize, local));
            }
            sock.state = SocketState::Listening;

            // Phase 5B: Register listener in TCP demux table.
            let _ = tcp_socket::TCP_DEMUX
                .lock()
                .register_listener(local.ip, local.port, sock_idx);

            // Phase 5B: Set bidirectional link on the connection.
            tcp::tcp_set_socket_idx(tcp_idx, Some(sock_idx as usize));

            0
        }
        Err(e) => map_tcp_err(e),
    }
}

pub fn socket_accept(sock_idx: u32, peer_addr: *mut [u8; 4], peer_port: *mut u16) -> i32 {
    loop {
        let (nonblocking, timeout_ms, accept_hint) = {
            let table = NEW_SOCKET_TABLE.lock();
            let Some(sock) = table.get(sock_idx as usize) else {
                return errno_i32(ERRNO_ENOTSOCK);
            };
            if sock.state != SocketState::Listening {
                return errno_i32(ERRNO_EINVAL);
            }
            (
                sock.is_nonblocking(),
                sock.options.recv_timeout.unwrap_or(0),
                sock.accept_wq_idx,
            )
        };

        {
            let mut table = NEW_SOCKET_TABLE.lock();
            let Some(listen_sock) = table.get_mut(sock_idx as usize) else {
                return errno_i32(ERRNO_ENOTSOCK);
            };
            let listen_opts = SocketOptions {
                reuse_addr: listen_sock.options.reuse_addr,
                recv_buf_size: listen_sock.options.recv_buf_size,
                send_buf_size: listen_sock.options.send_buf_size,
                recv_timeout: listen_sock.options.recv_timeout,
                send_timeout: listen_sock.options.send_timeout,
                keepalive: listen_sock.options.keepalive,
                tcp_nodelay: listen_sock.options.tcp_nodelay,
            };
            let is_nonblocking = listen_sock.is_nonblocking();

            // Phase 5C: Dequeue from the TcpListenState accept queue.
            let accepted = if let SocketInner::Tcp(ref mut tcp_inner) = listen_sock.inner {
                tcp_inner.listen.as_mut().and_then(|ls| ls.accept())
            } else {
                None
            };

            if let Some(accepted_conn) = accepted {
                // Find the TCP connection index for this accepted connection.
                let tcp_idx = tcp::tcp_find(&accepted_conn.tuple);

                let Some(new_idx) = table.alloc(SocketInner::Tcp(TcpSocketInner {
                    conn_id: tcp_idx.map(|i| i as u32),
                    listen: None,
                })) else {
                    return errno_i32(ERRNO_ENOMEM);
                };

                let Some(sock) = table.get_mut(new_idx) else {
                    return errno_i32(ERRNO_ENOMEM);
                };
                sock.state = SocketState::Connected;
                sock.local_addr = Some(SockAddr::new(
                    Ipv4Addr(accepted_conn.tuple.local_ip),
                    Port(accepted_conn.tuple.local_port),
                ));
                sock.remote_addr = Some(SockAddr::new(
                    Ipv4Addr(accepted_conn.tuple.remote_ip),
                    Port(accepted_conn.tuple.remote_port),
                ));
                sock.options = listen_opts;
                sock.set_nonblocking(is_nonblocking);

                if !peer_addr.is_null() {
                    unsafe {
                        *peer_addr = accepted_conn.tuple.remote_ip;
                    }
                }
                if !peer_port.is_null() {
                    unsafe {
                        *peer_port = accepted_conn.tuple.remote_port;
                    }
                }

                // Phase 5B: Set bidirectional socket↔connection link.
                if let Some(tcp_idx) = tcp_idx {
                    tcp::tcp_set_socket_idx(tcp_idx, Some(new_idx));
                }

                return new_idx as i32;
            }
        }

        if nonblocking {
            return errno_i32(ERRNO_EAGAIN);
        }

        // Phase 5C: Wait for accept queue to become non-empty.
        let wait_ok = if timeout_ms > 0 {
            ACCEPT_WQS[wq_slot(accept_hint)].wait_event_timeout(
                || {
                    let table = NEW_SOCKET_TABLE.lock();
                    let Some(sock) = table.get(sock_idx as usize) else {
                        return true;
                    };
                    if let SocketInner::Tcp(ref tcp_inner) = sock.inner {
                        tcp_inner
                            .listen
                            .as_ref()
                            .map(|ls| ls.accept_queue_len() > 0)
                            .unwrap_or(false)
                    } else {
                        true
                    }
                },
                timeout_ms,
            )
        } else {
            ACCEPT_WQS[wq_slot(accept_hint)].wait_event(|| {
                let table = NEW_SOCKET_TABLE.lock();
                let Some(sock) = table.get(sock_idx as usize) else {
                    return true;
                };
                if let SocketInner::Tcp(ref tcp_inner) = sock.inner {
                    tcp_inner
                        .listen
                        .as_ref()
                        .map(|ls| ls.accept_queue_len() > 0)
                        .unwrap_or(false)
                } else {
                    true
                }
            })
        };

        if !wait_ok {
            return errno_i32(ERRNO_EAGAIN);
        }
    }
}

pub fn socket_connect(sock_idx: u32, addr: [u8; 4], port: u16) -> i32 {
    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };

    match &mut sock.inner {
        SocketInner::Tcp(tcp_inner) => {
            if matches!(sock.state, SocketState::Connected | SocketState::Connecting) {
                return errno_i32(ERRNO_EISCONN);
            }

            let local_ip = sock.local_addr.map(|a| a.ip.0).unwrap_or_else(|| {
                crate::net::netstack::NET_STACK
                    .first_ipv4()
                    .map(|ip| ip.0)
                    .unwrap_or([0; 4])
            });

            match tcp::tcp_connect(local_ip, addr, port) {
                Ok((tcp_idx, syn)) => {
                    let send_rc = socket_send_tcp_segment(&syn, &[]);
                    if send_rc != 0 {
                        let _ = tcp::tcp_abort(tcp_idx);
                        return send_rc;
                    }

                    sock.local_addr = Some(SockAddr::new(
                        Ipv4Addr(syn.tuple.local_ip),
                        Port(syn.tuple.local_port),
                    ));
                    sock.remote_addr = Some(SockAddr::new(Ipv4Addr(addr), Port(port)));
                    tcp_inner.conn_id = Some(tcp_idx as u32);
                    sock.state = SocketState::Connecting;

                    // Phase 5B: Set bidirectional socket↔connection link.
                    tcp::tcp_set_socket_idx(tcp_idx, Some(sock_idx as usize));
                    0
                }
                Err(e) => map_tcp_err(e),
            }
        }
        SocketInner::Udp(_) => {
            sock.remote_addr = Some(SockAddr::new(Ipv4Addr(addr), Port(port)));
            sock.state = SocketState::Connected;
            0
        }
        SocketInner::Raw(_) => errno_i32(ERRNO_EPROTONOSUPPORT),
    }
}

pub fn socket_send(sock_idx: u32, data: *const u8, len: usize) -> i64 {
    if data.is_null() && len != 0 {
        return errno_i32(ERRNO_EFAULT) as i64;
    }

    let is_udp = {
        let table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        if sock.is_write_shutdown() {
            return errno_i32(ERRNO_EPIPE) as i64;
        }
        socket_is_udp(sock)
    };

    if is_udp {
        if len > UDP_DGRAM_MAX_PAYLOAD {
            return errno_i32(ERRNO_EINVAL) as i64;
        }

        let mut auto_bind: Option<(SockAddr, bool)> = None;
        let (local, remote, state) = {
            let mut table = NEW_SOCKET_TABLE.lock();
            let Some(sock) = table.get_mut(sock_idx as usize) else {
                return errno_i32(ERRNO_ENOTSOCK) as i64;
            };

            if sock.local_addr.is_none() || sock.local_addr.map(|a| a.port.0 == 0).unwrap_or(true) {
                let Some(port) = alloc_ephemeral_port() else {
                    return errno_i32(ERRNO_ENOMEM) as i64;
                };
                let local_ip = crate::net::netstack::NET_STACK
                    .first_ipv4()
                    .map(|ip| ip.0)
                    .unwrap_or([0; 4]);
                let local = SockAddr::new(Ipv4Addr(local_ip), port);
                sock.local_addr = Some(local);
                if sock.state == SocketState::Unbound {
                    sock.state = SocketState::Bound;
                }
                auto_bind = Some((local, sock.options.reuse_addr));
            }

            let local = match sock.local_addr {
                Some(v) => v,
                None => return errno_i32(ERRNO_ENOTCONN) as i64,
            };
            let remote = match sock.remote_addr {
                Some(v) => v,
                None => return errno_i32(ERRNO_ENOTCONN) as i64,
            };
            (local, remote, sock.state)
        };

        if let Some((bind_addr, reuse_addr)) = auto_bind
            && let Err(err) =
                crate::net::udp::udp_bind(sock_idx, bind_addr.ip, bind_addr.port, reuse_addr)
        {
            let mut table = NEW_SOCKET_TABLE.lock();
            if let Some(sock) = table.get_mut(sock_idx as usize)
                && socket_is_udp(sock)
                && sock.local_addr == Some(bind_addr)
                && sock.state == SocketState::Bound
            {
                sock.local_addr = None;
                sock.state = SocketState::Unbound;
            }
            EPHEMERAL_PORTS.lock().release(bind_addr.port);
            return map_net_err(err) as i64;
        }

        if state != SocketState::Connected {
            return errno_i32(ERRNO_ENOTCONN) as i64;
        }

        let payload = if len == 0 {
            &[][..]
        } else {
            unsafe { core::slice::from_raw_parts(data, len) }
        };

        return match crate::net::udp::udp_sendto(
            local.ip.0,
            remote.ip.0,
            local.port.0,
            remote.port.0,
            payload,
        ) {
            Ok(n) => n as i64,
            Err(err) => map_net_err(err) as i64,
        };
    }

    let (tcp_idx, state, nonblocking, timeout_ms, send_hint) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        sync_socket_state(sock);
        (
            socket_tcp_conn_id(sock),
            sock.state,
            sock.is_nonblocking(),
            sock.options.send_timeout.unwrap_or(0),
            sock.send_wq_idx,
        )
    };

    if !matches!(state, SocketState::Connected) {
        return errno_i32(ERRNO_ENOTCONN) as i64;
    }
    let Some(tcp_idx) = tcp_idx else {
        return errno_i32(ERRNO_ENOTCONN) as i64;
    };

    let payload = if len == 0 {
        &[][..]
    } else {
        unsafe { core::slice::from_raw_parts(data, len) }
    };

    let mut total_wrote = 0usize;
    while total_wrote < payload.len() {
        let space = tcp::tcp_send_buffer_space(tcp_idx);
        if space == 0 {
            if total_wrote > 0 {
                break;
            }
            if nonblocking {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }
            let wait_ok = if timeout_ms > 0 {
                SEND_WQS[wq_slot(send_hint)]
                    .wait_event_timeout(|| tcp::tcp_send_buffer_space(tcp_idx) > 0, timeout_ms)
            } else {
                SEND_WQS[wq_slot(send_hint)].wait_event(|| tcp::tcp_send_buffer_space(tcp_idx) > 0)
            };
            if !wait_ok {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }
            continue;
        }

        let remaining = payload.len() - total_wrote;
        let chunk_len = cmp::min(space, remaining);
        let chunk = &payload[total_wrote..total_wrote + chunk_len];
        let wrote = match tcp::tcp_send(tcp_idx, chunk) {
            Ok(n) => n,
            Err(e) => {
                if total_wrote > 0 {
                    break;
                }
                return map_tcp_err_i64(e);
            }
        };

        if wrote == 0 {
            if total_wrote > 0 {
                break;
            }
            if nonblocking {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }
            let wait_ok = if timeout_ms > 0 {
                SEND_WQS[wq_slot(send_hint)]
                    .wait_event_timeout(|| tcp::tcp_send_buffer_space(tcp_idx) > 0, timeout_ms)
            } else {
                SEND_WQS[wq_slot(send_hint)].wait_event(|| tcp::tcp_send_buffer_space(tcp_idx) > 0)
            };
            if !wait_ok {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }
            continue;
        }
        total_wrote += wrote;
    }

    let mut tx_payload = [0u8; TCP_TX_MAX];
    let now_ms = slopos_lib::clock::uptime_ms();
    loop {
        let Some((seg, n)) = tcp::tcp_poll_transmit(tcp_idx, &mut tx_payload, now_ms) else {
            break;
        };
        let rc = socket_send_tcp_segment(&seg, &tx_payload[..n]);
        if rc != 0 {
            return rc as i64;
        }
    }

    total_wrote as i64
}

pub fn socket_recv(sock_idx: u32, buf: *mut u8, len: usize) -> i64 {
    if buf.is_null() && len != 0 {
        return errno_i32(ERRNO_EFAULT) as i64;
    }

    let out = if len == 0 {
        &mut [][..]
    } else {
        unsafe { core::slice::from_raw_parts_mut(buf, len) }
    };

    let (is_udp, is_shut_rd) = {
        let table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        (socket_is_udp(sock), sock.is_read_shutdown())
    };

    if is_shut_rd {
        // SHUT_RD: return EOF (0) for both UDP and TCP.
        return 0;
    }

    if is_udp {
        let (nonblocking, timeout_ms, recv_hint, peer_filter) = {
            let table = NEW_SOCKET_TABLE.lock();
            let Some(sock) = table.get(sock_idx as usize) else {
                return errno_i32(ERRNO_ENOTSOCK) as i64;
            };
            let peer = if sock.state == SocketState::Connected {
                sock.remote_addr
            } else {
                None
            };
            (
                sock.is_nonblocking(),
                sock.options.recv_timeout.unwrap_or(0),
                sock.recv_wq_idx,
                peer,
            )
        };

        loop {
            let packet = {
                let mut table = NEW_SOCKET_TABLE.lock();
                let Some(sock) = table.get_mut(sock_idx as usize) else {
                    return errno_i32(ERRNO_ENOTSOCK) as i64;
                };

                let mut found = None;
                while let Some((pkt, src)) = sock.recv_queue.pop() {
                    if let Some(peer) = peer_filter
                        && src != peer
                    {
                        continue;
                    }
                    found = Some((pkt, src));
                    break;
                }
                found
            };

            if let Some((pkt, _src)) = packet {
                let payload = pkt.payload();
                let copy_len = cmp::min(out.len(), payload.len());
                out[..copy_len].copy_from_slice(&payload[..copy_len]);
                return copy_len as i64;
            }

            if nonblocking {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }

            let wait_ok = if timeout_ms > 0 {
                RECV_WQS[wq_slot(recv_hint)].wait_event_timeout(
                    || {
                        let table = NEW_SOCKET_TABLE.lock();
                        table
                            .get(sock_idx as usize)
                            .map(|sock| !sock.recv_queue.is_empty())
                            .unwrap_or(true)
                    },
                    timeout_ms,
                )
            } else {
                RECV_WQS[wq_slot(recv_hint)].wait_event(|| {
                    let table = NEW_SOCKET_TABLE.lock();
                    table
                        .get(sock_idx as usize)
                        .map(|sock| !sock.recv_queue.is_empty())
                        .unwrap_or(true)
                })
            };

            if !wait_ok {
                return errno_i32(ERRNO_EAGAIN) as i64;
            }
        }
    }

    let (tcp_idx, state, nonblocking, timeout_ms, recv_hint) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK) as i64;
        };
        sync_socket_state(sock);
        (
            socket_tcp_conn_id(sock),
            sock.state,
            sock.is_nonblocking(),
            sock.options.recv_timeout.unwrap_or(0),
            sock.recv_wq_idx,
        )
    };

    if !matches!(state, SocketState::Connected | SocketState::Connecting) {
        return errno_i32(ERRNO_ENOTCONN) as i64;
    }

    let Some(tcp_idx) = tcp_idx else {
        return errno_i32(ERRNO_ENOTCONN) as i64;
    };

    loop {
        match tcp::tcp_recv(tcp_idx, out) {
            Ok(n) => {
                if n > 0 {
                    return n as i64;
                }

                // EOF conditions:
                // 1. Connection is in a post-FIN state (FinWait1/2, Closing, etc.).
                // 2. Peer sent FIN (CloseWait/LastAck) and recv buffer is drained.
                if !matches!(
                    tcp::tcp_get_state(tcp_idx),
                    Some(TcpState::Established | TcpState::CloseWait)
                ) || tcp::tcp_is_peer_closed(tcp_idx)
                {
                    return 0;
                }

                if nonblocking {
                    return errno_i32(ERRNO_EAGAIN) as i64;
                }

                let wait_ok = if timeout_ms > 0 {
                    RECV_WQS[wq_slot(recv_hint)].wait_event_timeout(
                        || {
                            tcp::tcp_recv_available(tcp_idx) > 0
                                || tcp::tcp_is_peer_closed(tcp_idx)
                                || !matches!(
                                    tcp::tcp_get_state(tcp_idx),
                                    Some(TcpState::Established | TcpState::CloseWait)
                                )
                        },
                        timeout_ms,
                    )
                } else {
                    RECV_WQS[wq_slot(recv_hint)].wait_event(|| {
                        tcp::tcp_recv_available(tcp_idx) > 0
                            || tcp::tcp_is_peer_closed(tcp_idx)
                            || !matches!(
                                tcp::tcp_get_state(tcp_idx),
                                Some(TcpState::Established | TcpState::CloseWait)
                            )
                    })
                };

                if !wait_ok {
                    return errno_i32(ERRNO_EAGAIN) as i64;
                }
            }
            Err(e) => return map_tcp_err_i64(e),
        }
    }
}

pub fn socket_close(sock_idx: u32) -> i32 {
    let (tcp_idx, udp_unbind, recv_hint, send_hint, accept_hint, was_listener) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK);
        };

        let tcp_idx = socket_tcp_conn_id(sock);
        let udp_unbind = if socket_is_udp(sock) {
            sock.local_addr
        } else {
            None
        };
        let was_listener = sock.state == SocketState::Listening;
        let recv_hint = sock.recv_wq_idx;
        let send_hint = sock.send_wq_idx;
        let accept_hint = sock.accept_wq_idx;
        sock.recv_queue.clear();

        // Phase 5C: Clean up TcpListenState (cancels SYN-ACK retransmit timers).
        if let SocketInner::Tcp(ref mut tcp_inner) = sock.inner {
            if let Some(ref mut listen_state) = tcp_inner.listen {
                listen_state.clear();
            }
            tcp_inner.listen = None;
        }

        table.free(sock_idx as usize);
        (
            tcp_idx,
            udp_unbind,
            recv_hint,
            send_hint,
            accept_hint,
            was_listener,
        )
    };

    // Phase 5B: Unregister from TCP demux table.
    if was_listener {
        tcp_socket::TCP_DEMUX.lock().unregister_listener(sock_idx);
    }
    if let Some(tcp_idx) = tcp_idx {
        tcp_socket::TCP_DEMUX
            .lock()
            .unregister_established(tcp_idx as u32);
        // Clear the bidirectional link.
        tcp::tcp_set_socket_idx(tcp_idx, None);
    }

    if let Some(local) = udp_unbind {
        crate::net::udp::udp_unbind(sock_idx, local.ip, local.port);
        EPHEMERAL_PORTS.lock().release(local.port);
    }

    socket_wake_recv_hint(recv_hint);
    socket_wake_send_hint(send_hint);
    socket_wake_accept_hint(accept_hint);

    if let Some(tcp_idx) = tcp_idx {
        match tcp::tcp_close(tcp_idx) {
            Ok(Some(seg)) => {
                let _ = socket_send_tcp_segment(&seg, &[]);
                socket_notify_tcp_idx_waiters(tcp_idx);
                0
            }
            Ok(None) => 0,
            Err(e) => map_tcp_err(e),
        }
    } else {
        0
    }
}

pub fn socket_poll_readable(sock_idx: u32) -> u32 {
    let (state, is_udp, tcp_idx, has_udp_data) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return 0;
        };
        sync_socket_state(sock);
        (
            sock.state,
            socket_is_udp(sock),
            socket_tcp_conn_id(sock),
            !sock.recv_queue.is_empty(),
        )
    };

    if state == SocketState::Listening {
        // Phase 5C: Check accept queue in TcpListenState.
        let table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get(sock_idx as usize) else {
            return 0;
        };
        let has_pending = if let SocketInner::Tcp(ref tcp_inner) = sock.inner {
            tcp_inner
                .listen
                .as_ref()
                .map(|ls| ls.accept_queue_len() > 0)
                .unwrap_or(false)
        } else {
            false
        };
        if has_pending {
            return POLLIN as u32;
        }
        return 0;
    }

    if is_udp {
        return if has_udp_data { POLLIN as u32 } else { 0 };
    }

    let Some(tcp_idx) = tcp_idx else {
        return 0;
    };

    let mut flags = 0u32;
    if tcp::tcp_recv_available(tcp_idx) > 0 {
        flags |= POLLIN as u32;
    }

    match tcp::tcp_get_state(tcp_idx) {
        Some(TcpState::Established | TcpState::CloseWait) => {}
        Some(
            TcpState::FinWait1
            | TcpState::FinWait2
            | TcpState::Closing
            | TcpState::LastAck
            | TcpState::TimeWait,
        ) => {
            flags |= POLLHUP as u32;
        }
        Some(TcpState::Closed) | None => {
            flags |= (POLLERR | POLLHUP) as u32;
        }
        _ => {}
    }

    flags
}

pub fn socket_poll_writable(sock_idx: u32) -> u32 {
    let (is_udp, tcp_idx, state) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return 0;
        };
        sync_socket_state(sock);
        (socket_is_udp(sock), socket_tcp_conn_id(sock), sock.state)
    };

    if is_udp {
        return POLLOUT as u32;
    }

    let Some(tcp_idx) = tcp_idx else {
        return 0;
    };

    let mut flags = 0u32;
    if matches!(state, SocketState::Connected) && tcp::tcp_send_buffer_space(tcp_idx) > 0 {
        flags |= POLLOUT as u32;
    }

    match tcp::tcp_get_state(tcp_idx) {
        Some(TcpState::Established | TcpState::CloseWait) => {}
        Some(TcpState::Closed) | None => {
            flags |= (POLLERR | POLLHUP) as u32;
        }
        Some(
            TcpState::FinWait1
            | TcpState::FinWait2
            | TcpState::Closing
            | TcpState::LastAck
            | TcpState::TimeWait,
        ) => {
            flags |= POLLHUP as u32;
        }
        _ => {}
    }

    flags
}

pub fn socket_get_state(sock_idx: u32) -> Option<SocketState> {
    NEW_SOCKET_TABLE
        .lock()
        .get(sock_idx as usize)
        .map(|s| s.state)
}

pub fn socket_set_nonblocking(sock_idx: u32, nonblocking: bool) -> i32 {
    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };
    sock.set_nonblocking(nonblocking);
    0
}

pub fn socket_set_timeouts(sock_idx: u32, recv_timeout_ms: u64, send_timeout_ms: u64) -> i32 {
    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };
    sock.options.recv_timeout = if recv_timeout_ms == 0 {
        None
    } else {
        Some(recv_timeout_ms)
    };
    sock.options.send_timeout = if send_timeout_ms == 0 {
        None
    } else {
        Some(send_timeout_ms)
    };
    0
}

pub fn socket_reset_all() {
    {
        let mut table = NEW_SOCKET_TABLE.lock();
        table.init_if_needed();
        let cap = table.capacity();
        for idx in 0..cap {
            if let Some(sock) = table.get(idx) {
                socket_wake_recv_hint(sock.recv_wq_idx);
                socket_wake_accept_hint(sock.accept_wq_idx);
                socket_wake_send_hint(sock.send_wq_idx);
            }
            table.free(idx);
        }
    }

    for idx in 0..MAX_SOCKETS {
        RECV_WQS[idx].wake_all();
        ACCEPT_WQS[idx].wake_all();
        SEND_WQS[idx].wake_all();
    }

    *EPHEMERAL_PORTS.lock() = EphemeralPortAllocator::new();
    crate::net::udp::UDP_DEMUX.lock().clear();
    tcp::tcp_reset_all();
}

#[derive(Clone, Copy)]
pub struct SocketSnapshot {
    pub state: SocketState,
    pub local_ip: [u8; 4],
    pub local_port: u16,
    pub remote_ip: [u8; 4],
    pub remote_port: u16,
    pub nonblocking: bool,
}

pub fn socket_snapshot(sock_idx: u32) -> Option<SocketSnapshot> {
    NEW_SOCKET_TABLE.lock().get(sock_idx as usize).map(|sock| {
        let local = sock
            .local_addr
            .unwrap_or(SockAddr::new(Ipv4Addr::UNSPECIFIED, Port(0)));
        let remote = sock
            .remote_addr
            .unwrap_or(SockAddr::new(Ipv4Addr::UNSPECIFIED, Port(0)));
        SocketSnapshot {
            state: sock.state,
            local_ip: local.ip.0,
            local_port: local.port.0,
            remote_ip: remote.ip.0,
            remote_port: remote.port.0,
            nonblocking: sock.is_nonblocking(),
        }
    })
}

pub fn socket_lookup_tcp_idx(sock_idx: u32) -> Option<usize> {
    NEW_SOCKET_TABLE
        .lock()
        .get(sock_idx as usize)
        .and_then(socket_tcp_conn_id)
}

pub fn socket_count_active() -> usize {
    NEW_SOCKET_TABLE.lock().count_active()
}

pub fn socket_setsockopt(sock_idx: u32, level: i32, optname: i32, val: &[u8]) -> i32 {
    use slopos_abi::syscall::*;

    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };

    match level {
        SOL_SOCKET => match optname {
            SO_REUSEADDR => {
                if val.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = i32::from_ne_bytes([val[0], val[1], val[2], val[3]]);
                sock.options.reuse_addr = v != 0;
                0
            }
            SO_RCVBUF => {
                if val.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = u32::from_ne_bytes([val[0], val[1], val[2], val[3]]) as usize;
                let Ok(size) = SocketOptions::validate_recv_buf_size(v) else {
                    return errno_i32(ERRNO_EINVAL);
                };
                sock.options.recv_buf_size = size;
                sock.recv_queue.resize(size);
                0
            }
            SO_SNDBUF => {
                if val.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = u32::from_ne_bytes([val[0], val[1], val[2], val[3]]) as usize;
                let Ok(size) = SocketOptions::validate_send_buf_size(v) else {
                    return errno_i32(ERRNO_EINVAL);
                };
                sock.options.send_buf_size = size;
                0
            }
            SO_RCVTIMEO => {
                if val.len() < 8 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let ms = u64::from_ne_bytes([
                    val[0], val[1], val[2], val[3], val[4], val[5], val[6], val[7],
                ]);
                sock.options.recv_timeout = if ms == 0 { None } else { Some(ms) };
                0
            }
            SO_SNDTIMEO => {
                if val.len() < 8 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let ms = u64::from_ne_bytes([
                    val[0], val[1], val[2], val[3], val[4], val[5], val[6], val[7],
                ]);
                sock.options.send_timeout = if ms == 0 { None } else { Some(ms) };
                0
            }
            SO_KEEPALIVE => {
                if val.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = i32::from_ne_bytes([val[0], val[1], val[2], val[3]]);
                sock.options.keepalive = v != 0;
                0
            }
            _ => errno_i32(ERRNO_EINVAL),
        },
        IPPROTO_TCP => match optname {
            TCP_NODELAY => {
                if val.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = i32::from_ne_bytes([val[0], val[1], val[2], val[3]]);
                sock.options.tcp_nodelay = v != 0;
                0
            }
            _ => errno_i32(ERRNO_EINVAL),
        },
        _ => errno_i32(ERRNO_EINVAL),
    }
}

pub fn socket_getsockopt(sock_idx: u32, level: i32, optname: i32, out: &mut [u8]) -> i32 {
    use slopos_abi::syscall::*;

    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };

    match level {
        SOL_SOCKET => match optname {
            SO_REUSEADDR => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v: i32 = if sock.options.reuse_addr { 1 } else { 0 };
                out[..4].copy_from_slice(&v.to_ne_bytes());
                4
            }
            SO_ERROR => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let err = sock.take_pending_error().map(map_net_err).unwrap_or(0);
                out[..4].copy_from_slice(&err.to_ne_bytes());
                4
            }
            SO_RCVBUF => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = sock.options.recv_buf_size as u32;
                out[..4].copy_from_slice(&v.to_ne_bytes());
                4
            }
            SO_SNDBUF => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = sock.options.send_buf_size as u32;
                out[..4].copy_from_slice(&v.to_ne_bytes());
                4
            }
            SO_RCVTIMEO => {
                if out.len() < 8 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = sock.options.recv_timeout.unwrap_or(0);
                out[..8].copy_from_slice(&v.to_ne_bytes());
                8
            }
            SO_SNDTIMEO => {
                if out.len() < 8 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v = sock.options.send_timeout.unwrap_or(0);
                out[..8].copy_from_slice(&v.to_ne_bytes());
                8
            }
            SO_KEEPALIVE => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v: i32 = if sock.options.keepalive { 1 } else { 0 };
                out[..4].copy_from_slice(&v.to_ne_bytes());
                4
            }
            _ => errno_i32(ERRNO_EINVAL),
        },
        IPPROTO_TCP => match optname {
            TCP_NODELAY => {
                if out.len() < 4 {
                    return errno_i32(ERRNO_EINVAL);
                }
                let v: i32 = if sock.options.tcp_nodelay { 1 } else { 0 };
                out[..4].copy_from_slice(&v.to_ne_bytes());
                4
            }
            _ => errno_i32(ERRNO_EINVAL),
        },
        _ => errno_i32(ERRNO_EINVAL),
    }
}

pub fn socket_shutdown(sock_idx: u32, how: i32) -> i32 {
    use slopos_abi::syscall::*;

    let (tcp_idx, recv_hint) = {
        let mut table = NEW_SOCKET_TABLE.lock();
        let Some(sock) = table.get_mut(sock_idx as usize) else {
            return errno_i32(ERRNO_ENOTSOCK);
        };

        let tcp_idx = socket_tcp_conn_id(sock);
        let recv_hint = sock.recv_wq_idx;

        match how {
            SHUT_RD => {
                sock.flags.set(SocketFlags::SHUT_RD);
                if socket_is_udp(sock) {
                    sock.recv_queue.clear();
                }
            }
            SHUT_WR => {
                sock.flags.set(SocketFlags::SHUT_WR);
            }
            SHUT_RDWR => {
                sock.flags.set(SocketFlags::SHUT_RD);
                sock.flags.set(SocketFlags::SHUT_WR);
                if socket_is_udp(sock) {
                    sock.recv_queue.clear();
                }
            }
            _ => return errno_i32(ERRNO_EINVAL),
        }

        (tcp_idx, recv_hint)
    };

    // For TCP sockets, perform protocol-level shutdown actions.
    if let Some(tcp_idx) = tcp_idx {
        let shut_wr = how == SHUT_WR || how == SHUT_RDWR;
        let shut_rd = how == SHUT_RD || how == SHUT_RDWR;

        if shut_wr {
            if let Ok(Some(seg)) = tcp::tcp_shutdown_write(tcp_idx) {
                let _ = socket_send_tcp_segment(&seg, &[]);
            }
        }

        if shut_rd {
            tcp::tcp_recv_discard(tcp_idx);
            // Wake recv waiters so they see EOF.
            socket_wake_recv_hint(recv_hint);
        }
    }

    0
}

pub fn socket_send_queued(sock_idx: u32) -> i32 {
    let tcp_idx = match socket_lookup_tcp_idx(sock_idx) {
        Some(i) => i,
        None => return errno_i32(ERRNO_ENOTCONN),
    };

    let mut tx_payload = [0u8; TCP_TX_MAX];
    let now_ms = slopos_lib::clock::uptime_ms();
    loop {
        let Some((seg, n)) = tcp::tcp_poll_transmit(tcp_idx, &mut tx_payload, now_ms) else {
            break;
        };
        let rc = socket_send_tcp_segment(&seg, &tx_payload[..n]);
        if rc != 0 {
            return rc;
        }
    }
    0
}

pub fn socket_process_timers() {
    let now_ms = slopos_lib::clock::uptime_ms();
    if let Some(idx) = tcp::tcp_retransmit_check(now_ms)
        && let Some(sock_idx) = socket_from_tcp_idx(idx)
    {
        let _ = socket_send_queued(sock_idx);
    }

    if let Some((_idx, seg)) = tcp::tcp_delayed_ack_check(now_ms) {
        let _ = socket_send_tcp_segment(&seg, &[]);
    }
}

fn socket_from_tcp_idx(tcp_idx: usize) -> Option<u32> {
    let table = NEW_SOCKET_TABLE.lock();
    for (idx, sock) in table.slots.iter().enumerate() {
        if let Some(sock) = sock
            && socket_tcp_conn_id(sock) == Some(tcp_idx)
        {
            return Some(idx as u32);
        }
    }
    None
}

pub fn socket_debug_set_connected(sock_idx: u32, remote_ip: [u8; 4], remote_port: u16) -> i32 {
    let mut table = NEW_SOCKET_TABLE.lock();
    let Some(sock) = table.get_mut(sock_idx as usize) else {
        return errno_i32(ERRNO_ENOTSOCK);
    };
    let Some(tcp_idx) = socket_tcp_conn_id(sock) else {
        return errno_i32(ERRNO_ENOTCONN);
    };

    if let Some(conn) = tcp::tcp_get_connection(tcp_idx)
        && conn.state == TcpState::Established
    {
        sock.state = SocketState::Connected;
        sock.remote_addr = Some(SockAddr::new(Ipv4Addr(remote_ip), Port(remote_port)));
        return 0;
    }
    errno_i32(ERRNO_ENOTCONN)
}

pub fn socket_host_to_be_port(port: u16) -> u16 {
    u16::from_be_bytes(be_port(port))
}

pub fn socket_be_to_host_port(port: u16) -> u16 {
    u16::from_be(port)
}

pub fn socket_max_send_probe(sock_idx: u32, max_len: usize) -> i32 {
    let Some(tcp_idx) = socket_lookup_tcp_idx(sock_idx) else {
        return errno_i32(ERRNO_ENOTCONN);
    };
    let space = tcp::tcp_send_buffer_space(tcp_idx);
    cmp::min(space, max_len) as i32
}
