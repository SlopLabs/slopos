extern crate alloc;

use core::ffi::c_void;
use core::mem::MaybeUninit;
use core::ptr;

use alloc::vec::Vec;

use slopos_abi::addr::{PhysAddr, VirtAddr};
use slopos_lib::cpu::msr::Msr;
use slopos_lib::testing::TestResult;
use slopos_lib::{assert_not_null, assert_test, cpu, fail, klog_info, pass};

use crate::hhdm::PhysAddrHhdm;
use crate::kernel_heap::{get_heap_stats, kfree, kmalloc, kzalloc};
use crate::page_alloc::{
    ALLOC_FLAG_ZERO, alloc_page_frame, alloc_page_frames, free_page_frame,
    get_page_allocator_stats, page_frame_get_ref, page_frame_inc_ref,
};
use crate::paging::{
    get_current_page_directory, paging_get_kernel_directory, paging_is_cow,
    paging_is_user_accessible, virt_to_phys,
};
use crate::paging_defs::PAGE_SIZE_4KB;
use crate::process_vm::get_process_vm_stats;

// ============================================================================
// PAGE ALLOCATOR (BUDDY) TESTS - 12 tests
// ============================================================================

/// Test 1: Allocate and free a single 4KB page
pub fn test_page_alloc_single() -> TestResult {
    let phys = alloc_page_frame(0);
    assert_not_null!(phys.as_u64() as *const u8, "allocate single page");
    assert_test!(phys.as_u64() != 0, "allocated address is zero");

    let ref_count = page_frame_get_ref(phys);
    if ref_count == 0 {
        free_page_frame(phys);
        return fail!(
            "ref count should be non-zero after alloc, got {}",
            ref_count
        );
    }

    free_page_frame(phys);
    pass!()
}

/// Test 2: Allocate multi-order blocks (2, 4, 8 pages)
pub fn test_page_alloc_multi_order() -> TestResult {
    let phys2 = alloc_page_frames(2, 0);
    assert_not_null!(phys2.as_u64() as *const u8, "allocate 2 pages");

    let phys4 = alloc_page_frames(4, 0);
    if phys4.is_null() {
        free_page_frame(phys2);
        return fail!("allocate 4 pages");
    }

    let phys8 = alloc_page_frames(8, 0);
    if phys8.is_null() {
        free_page_frame(phys2);
        free_page_frame(phys4);
        return fail!("allocate 8 pages");
    }

    free_page_frame(phys2);
    free_page_frame(phys4);
    free_page_frame(phys8);
    pass!()
}

/// Test 3: Alloc→free→alloc same size, verify address reuse (coalescing)
pub fn test_page_alloc_free_cycle() -> TestResult {
    let phys1 = alloc_page_frame(0);
    assert_not_null!(phys1.as_u64() as *const u8, "first alloc");

    free_page_frame(phys1);

    let phys2 = alloc_page_frame(0);
    assert_not_null!(phys2.as_u64() as *const u8, "second alloc after free");

    // With good coalescing, we might get the same address back (not guaranteed)
    // At minimum, the allocation should succeed
    free_page_frame(phys2);
    pass!()
}

/// Test 4: Allocate with ALLOC_FLAG_ZERO, verify memory is zeroed
pub fn test_page_alloc_zeroed() -> TestResult {
    let phys = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "allocate zeroed page");

    if let Some(virt) = phys.to_virt_checked() {
        let ptr: *const u8 = virt.as_ptr();
        for i in 0..64 {
            let byte = unsafe { *ptr.add(i) };
            if byte != 0 {
                klog_info!(
                    "PAGE_ALLOC_TEST: Zeroed page has non-zero byte at offset {}",
                    i
                );
                free_page_frame(phys);
                return fail!("zeroed page has non-zero byte at offset {}", i);
            }
        }
    }

    free_page_frame(phys);
    pass!()
}

/// Test 5: Reference count increment and decrement
pub fn test_page_alloc_refcount() -> TestResult {
    let phys = alloc_page_frame(0);
    assert_not_null!(phys.as_u64() as *const u8, "alloc for refcount test");

    let ref1 = page_frame_get_ref(phys);
    if ref1 != 1 {
        free_page_frame(phys);
        return fail!("initial refcount should be 1, got {}", ref1);
    }

    let new_ref = page_frame_inc_ref(phys);
    if new_ref != 2 {
        free_page_frame(phys);
        free_page_frame(phys);
        return fail!("refcount after inc should be 2, got {}", new_ref);
    }

    // First free should just decrement
    free_page_frame(phys);

    let ref_after = page_frame_get_ref(phys);
    if ref_after != 1 {
        free_page_frame(phys);
        return fail!("refcount after first free should be 1, got {}", ref_after);
    }

    // Second free should actually free
    free_page_frame(phys);
    pass!()
}

/// Test 6: Stats accuracy check
pub fn test_page_alloc_stats() -> TestResult {
    let mut total = 0u32;
    let mut free_before = 0u32;
    let mut alloc_before = 0u32;
    get_page_allocator_stats(&mut total, &mut free_before, &mut alloc_before);

    assert_test!(total != 0, "total frames is 0");

    let phys = alloc_page_frames(4, 0);
    assert_not_null!(phys.as_u64() as *const u8, "alloc 4 pages for stats");

    let mut free_after = 0u32;
    let mut alloc_after = 0u32;
    get_page_allocator_stats(ptr::null_mut(), &mut free_after, &mut alloc_after);

    if alloc_after < alloc_before + 4 {
        free_page_frame(phys);
        return fail!("allocated count didn't increase by 4");
    }

    free_page_frame(phys);
    pass!()
}

/// Test 7: Free NULL address should not crash
pub fn test_page_alloc_free_null() -> TestResult {
    // This should be a no-op, not crash
    let _result = free_page_frame(PhysAddr::NULL);
    pass!()
}

/// Test 8: Fragmentation stress test
pub fn test_page_alloc_fragmentation() -> TestResult {
    let mut pages: [PhysAddr; 8] = [PhysAddr::NULL; 8];
    for i in 0..8 {
        pages[i] = alloc_page_frame(0);
        if pages[i].is_null() {
            for j in 0..i {
                free_page_frame(pages[j]);
            }
            return fail!("failed to allocate page {}", i);
        }
    }

    // Free alternate pages (0, 2, 4, 6)
    free_page_frame(pages[0]);
    free_page_frame(pages[2]);
    free_page_frame(pages[4]);
    free_page_frame(pages[6]);

    // Try to allocate a 2-page block - may or may not succeed depending on layout
    let large = alloc_page_frames(2, 0);
    if !large.is_null() {
        free_page_frame(large);
    }

    // Free remaining
    free_page_frame(pages[1]);
    free_page_frame(pages[3]);
    free_page_frame(pages[5]);
    free_page_frame(pages[7]);
    pass!()
}

// ============================================================================
// KERNEL HEAP TESTS - 10 tests
// ============================================================================

/// Test 1: Small allocations (16, 32, 64 bytes)
pub fn test_heap_small_alloc() -> TestResult {
    let p16 = kmalloc(16);
    assert_not_null!(p16, "allocate 16 bytes");

    let p32 = kmalloc(32);
    if p32.is_null() {
        kfree(p16);
        return fail!("allocate 32 bytes");
    }

    let p64 = kmalloc(64);
    if p64.is_null() {
        kfree(p16);
        kfree(p32);
        return fail!("allocate 64 bytes");
    }

    kfree(p64);
    kfree(p32);
    kfree(p16);
    pass!()
}

/// Test 2: Medium allocations (256, 512, 1024 bytes)
pub fn test_heap_medium_alloc() -> TestResult {
    let p256 = kmalloc(256);
    assert_not_null!(p256, "allocate 256 bytes");

    let p512 = kmalloc(512);
    if p512.is_null() {
        kfree(p256);
        return fail!("allocate 512 bytes");
    }

    let p1k = kmalloc(1024);
    if p1k.is_null() {
        kfree(p256);
        kfree(p512);
        return fail!("allocate 1024 bytes");
    }

    kfree(p1k);
    kfree(p512);
    kfree(p256);
    pass!()
}

/// Test 3: Large allocations (4KB, 16KB)
pub fn test_heap_large_alloc() -> TestResult {
    let p4k = kmalloc(4096);
    assert_not_null!(p4k, "allocate 4KB");

    let p16k = kmalloc(16384);
    if p16k.is_null() {
        kfree(p4k);
        return fail!("allocate 16KB");
    }

    kfree(p16k);
    kfree(p4k);
    pass!()
}

/// Test 4: kzalloc returns zeroed memory
pub fn test_heap_kzalloc_zeroed() -> TestResult {
    let ptr = kzalloc(128);
    assert_not_null!(ptr, "kzalloc 128 bytes");

    let bytes = ptr as *const u8;
    for i in 0..128 {
        let b = unsafe { *bytes.add(i) };
        if b != 0 {
            kfree(ptr);
            return fail!("kzalloc memory not zeroed at offset {}", i);
        }
    }

    kfree(ptr);
    pass!()
}

/// Test 5: kfree(null) should not crash
pub fn test_heap_kfree_null() -> TestResult {
    kfree(ptr::null_mut());
    pass!()
}

/// Test 6: Allocation size zero should return null
pub fn test_heap_alloc_zero() -> TestResult {
    let ptr = kmalloc(0);
    if !ptr.is_null() {
        kfree(ptr);
        return fail!("kmalloc(0) should return null");
    }
    pass!()
}

/// Test 7: Stats tracking accuracy
pub fn test_heap_stats() -> TestResult {
    let mut stats_before = MaybeUninit::uninit();
    get_heap_stats(stats_before.as_mut_ptr());
    let before = unsafe { stats_before.assume_init() };

    let ptr = kmalloc(256);
    assert_not_null!(ptr, "alloc for stats test");

    let mut stats_after = MaybeUninit::uninit();
    get_heap_stats(stats_after.as_mut_ptr());
    let after = unsafe { stats_after.assume_init() };

    if after.allocated_size <= before.allocated_size {
        kfree(ptr);
        return fail!("allocated size didn't increase");
    }

    if after.allocation_count <= before.allocation_count {
        kfree(ptr);
        return fail!("allocation count didn't increase");
    }

    kfree(ptr);
    pass!()
}

pub fn test_global_alloc_vec() -> TestResult {
    let mut vec = Vec::new();
    for i in 0..128u64 {
        vec.push(i);
    }
    assert_test!(vec.len() == 128, "vec length should be 128");
    pass!()
}

pub fn test_heap_free_list_search() -> TestResult {
    let mut stats_before = MaybeUninit::uninit();
    get_heap_stats(stats_before.as_mut_ptr());
    let initial_heap_size = unsafe { stats_before.assume_init() }.total_size;

    let p1 = kmalloc(256);
    assert_not_null!(p1, "alloc p1");
    let p2 = kmalloc(256);
    if p2.is_null() {
        kfree(p1);
        return fail!("alloc p2");
    }
    let p3 = kmalloc(256);
    if p3.is_null() {
        kfree(p1);
        kfree(p2);
        return fail!("alloc p3");
    }

    let mut stats_after_alloc = MaybeUninit::uninit();
    get_heap_stats(stats_after_alloc.as_mut_ptr());
    let heap_after_alloc = unsafe { stats_after_alloc.assume_init() }.total_size;

    kfree(p1);
    kfree(p2);

    let p4 = kmalloc(256);
    if p4.is_null() {
        kfree(p3);
        return fail!("alloc p4");
    }
    let p5 = kmalloc(256);
    if p5.is_null() {
        kfree(p3);
        kfree(p4);
        return fail!("alloc p5");
    }

    let mut stats_final = MaybeUninit::uninit();
    get_heap_stats(stats_final.as_mut_ptr());
    let final_heap_size = unsafe { stats_final.assume_init() }.total_size;

    if final_heap_size > heap_after_alloc {
        kfree(p3);
        kfree(p4);
        kfree(p5);
        return fail!("heap grew beyond post-alloc size");
    }

    kfree(p3);
    kfree(p4);
    kfree(p5);

    assert_test!(
        final_heap_size >= initial_heap_size,
        "final heap size less than initial"
    );
    pass!()
}

/// Regression test: Verify HEAP_WARMUP_PAGES is sufficient for soft reboot coherency.
///
/// After soft reboot, x86 paging structure caches may retain stale entries. The fix
/// requires ≥2 physical frame allocations AND ≥1 page mapping during heap init.
/// This test ensures HEAP_WARMUP_PAGES is never reduced below the minimum threshold.
///
/// If this test fails, framebuffer performance will degrade to ~1 FPS after soft reboot.
/// See: Intel Application Note 317080-002 "TLBs, Paging-Structure Caches"
pub fn test_heap_warmup_pages_minimum() -> TestResult {
    use crate::kernel_heap::HEAP_WARMUP_PAGES;

    const MINIMUM_WARMUP_PAGES: u32 = 2;

    if HEAP_WARMUP_PAGES < MINIMUM_WARMUP_PAGES {
        return fail!(
            "HEAP_WARMUP_PAGES ({}) is below minimum ({}). \
             This WILL cause framebuffer performance regression after soft reboot!",
            HEAP_WARMUP_PAGES,
            MINIMUM_WARMUP_PAGES
        );
    }

    const RECOMMENDED_WARMUP_PAGES: u32 = 4;
    if HEAP_WARMUP_PAGES < RECOMMENDED_WARMUP_PAGES {
        klog_info!(
            "HEAP_TEST: Warning - HEAP_WARMUP_PAGES ({}) is below recommended ({})",
            HEAP_WARMUP_PAGES,
            RECOMMENDED_WARMUP_PAGES
        );
    }

    pass!()
}

pub fn test_heap_fragmentation_behind_head() -> TestResult {
    let mut ptrs: [*mut c_void; 5] = [ptr::null_mut(); 5];
    let sizes = [128usize, 256, 128, 512, 256];

    for (i, size) in sizes.iter().enumerate() {
        ptrs[i] = kmalloc(*size);
        if ptrs[i].is_null() {
            for j in 0..i {
                kfree(ptrs[j]);
            }
            return fail!("alloc {} bytes at index {}", size, i);
        }
    }

    kfree(ptrs[0]);
    kfree(ptrs[2]);
    kfree(ptrs[3]);

    let needed = kmalloc(400);
    if needed.is_null() {
        kfree(ptrs[1]);
        kfree(ptrs[4]);
        return fail!("alloc 400 bytes from freed gaps");
    }

    kfree(needed);
    kfree(ptrs[1]);
    kfree(ptrs[4]);
    pass!()
}

// ============================================================================
// PROCESS VM TESTS (existing)
// ============================================================================

use crate::process_vm::{
    create_process_vm, destroy_process_vm, init_process_vm, process_vm_get_page_dir,
};
use slopos_abi::task::INVALID_PROCESS_ID;

pub fn test_process_vm_slot_reuse() -> TestResult {
    init_process_vm();

    let mut initial_active: u32 = 0;
    get_process_vm_stats(ptr::null_mut(), &mut initial_active);

    let mut pids = [0u32; 5];
    for i in 0..5 {
        pids[i] = create_process_vm();
        if pids[i] == INVALID_PROCESS_ID {
            return fail!("create process {}", i);
        }
        if process_vm_get_page_dir(pids[i]).is_null() {
            return fail!("page dir for process {}", i);
        }
    }

    for &idx in &[1usize, 2, 3] {
        if destroy_process_vm(pids[idx]) != 0 {
            return fail!("destroy process at index {}", idx);
        }
    }

    for &idx in &[1usize, 2, 3] {
        if !process_vm_get_page_dir(pids[idx]).is_null() {
            return fail!("destroyed process {} should have null page dir", idx);
        }
    }

    assert_not_null!(process_vm_get_page_dir(pids[0]), "surviving process 0");
    assert_not_null!(process_vm_get_page_dir(pids[4]), "surviving process 4");

    let mut new_pids = [0u32; 3];
    for i in 0..3 {
        new_pids[i] = create_process_vm();
        if new_pids[i] == INVALID_PROCESS_ID {
            return fail!("create reuse process {}", i);
        }
        if process_vm_get_page_dir(new_pids[i]).is_null() {
            return fail!("reuse page dir {}", i);
        }
    }

    assert_not_null!(
        process_vm_get_page_dir(pids[0]),
        "original process 0 still alive"
    );
    assert_not_null!(
        process_vm_get_page_dir(pids[4]),
        "original process 4 still alive"
    );

    assert_test!(destroy_process_vm(pids[0]) == 0, "destroy original 0");
    assert_test!(destroy_process_vm(pids[4]) == 0, "destroy original 4");
    for pid in new_pids {
        destroy_process_vm(pid);
    }

    let mut final_active: u32 = 0;
    get_process_vm_stats(ptr::null_mut(), &mut final_active);
    if final_active != initial_active {
        return fail!(
            "active count mismatch: {} != {}",
            final_active,
            initial_active
        );
    }
    pass!()
}

pub fn test_process_vm_counter_reset() -> TestResult {
    init_process_vm();

    let mut initial_active: u32 = 0;
    get_process_vm_stats(ptr::null_mut(), &mut initial_active);

    let mut pids = [0u32; 10];
    for i in 0..10 {
        pids[i] = create_process_vm();
        if pids[i] == INVALID_PROCESS_ID {
            for j in 0..i {
                destroy_process_vm(pids[j]);
            }
            return fail!("create process {}", i);
        }
    }

    let mut active_after: u32 = 0;
    get_process_vm_stats(ptr::null_mut(), &mut active_after);
    if active_after != initial_active + 10 {
        for pid in pids {
            destroy_process_vm(pid);
        }
        return fail!(
            "active count should be {} + 10, got {}",
            initial_active,
            active_after
        );
    }

    for pid in pids.iter().rev() {
        if destroy_process_vm(*pid) != 0 {
            return fail!("destroy process {}", pid);
        }
    }

    let mut final_active: u32 = 0;
    get_process_vm_stats(ptr::null_mut(), &mut final_active);
    if final_active != initial_active {
        return fail!(
            "final active {} != initial {}",
            final_active,
            initial_active
        );
    }
    pass!()
}

// ============================================================================
// PAGING TESTS - 10 tests
// ============================================================================

/// Test 1: virt_to_phys on kernel address
pub fn test_paging_virt_to_phys() -> TestResult {
    let kernel_addr = VirtAddr::new(test_paging_virt_to_phys as *const () as u64);
    let phys = virt_to_phys(kernel_addr);
    assert_test!(
        !phys.is_null(),
        "virt_to_phys returned null for kernel code"
    );
    pass!()
}

/// Test 2: Kernel directory retrieval
pub fn test_paging_get_kernel_dir() -> TestResult {
    let kernel_dir = paging_get_kernel_directory();
    assert_not_null!(kernel_dir, "kernel directory");

    let current_dir = get_current_page_directory();
    assert_not_null!(current_dir, "current directory");
    pass!()
}

/// Test 3: User accessible check on kernel page (should fail)
pub fn test_paging_user_accessible_kernel() -> TestResult {
    let kernel_dir = paging_get_kernel_directory();
    assert_not_null!(kernel_dir, "kernel directory");

    let kernel_addr = VirtAddr::new(test_paging_user_accessible_kernel as *const () as u64);
    let is_user = paging_is_user_accessible(kernel_dir, kernel_addr);
    assert_test!(
        is_user == 0,
        "kernel code incorrectly marked as user accessible"
    );
    pass!()
}

/// Test 4: COW flag on kernel page (should not be set)
pub fn test_paging_cow_kernel() -> TestResult {
    let kernel_dir = paging_get_kernel_directory();
    assert_not_null!(kernel_dir, "kernel directory");

    let kernel_addr = VirtAddr::new(test_paging_cow_kernel as *const () as u64);
    let is_cow = paging_is_cow(kernel_dir, kernel_addr);
    assert_test!(!is_cow, "kernel code incorrectly marked as COW");
    pass!()
}

// ============================================================================
// RING BUFFER TESTS - 8 tests (in lib crate, tested via mm)
// ============================================================================

/// Test ring buffer basic push/pop
pub fn test_ring_buffer_basic() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 8> = RingBuffer::new();
    assert_test!(rb.is_empty(), "new buffer should be empty");
    assert_test!(rb.try_push(42), "push to empty buffer failed");
    assert_test!(!rb.is_empty(), "buffer should not be empty after push");

    let val = rb.try_pop();
    assert_test!(val == Some(42), "pop returned wrong value");
    assert_test!(rb.is_empty(), "buffer should be empty after pop");
    pass!()
}

/// Test ring buffer FIFO order
pub fn test_ring_buffer_fifo() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 8> = RingBuffer::new();
    rb.try_push(1);
    rb.try_push(2);
    rb.try_push(3);

    assert_test!(rb.try_pop() == Some(1), "FIFO order violated (expected 1)");
    assert_test!(rb.try_pop() == Some(2), "FIFO order violated (expected 2)");
    assert_test!(rb.try_pop() == Some(3), "FIFO order violated (expected 3)");
    pass!()
}

/// Test ring buffer empty pop
pub fn test_ring_buffer_empty_pop() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 8> = RingBuffer::new();
    assert_test!(rb.try_pop().is_none(), "pop from empty should return None");
    pass!()
}

/// Test ring buffer full behavior
pub fn test_ring_buffer_full() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 4> = RingBuffer::new();
    for i in 0..4 {
        if !rb.try_push(i) {
            return fail!("push {} failed unexpectedly", i);
        }
    }

    assert_test!(rb.is_full(), "buffer should be full");
    assert_test!(!rb.try_push(999), "push to full buffer should fail");
    pass!()
}

/// Test ring buffer overwrite mode
pub fn test_ring_buffer_overwrite() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 4> = RingBuffer::new();
    for i in 0..4u32 {
        rb.push_overwrite(i);
    }

    // Push 99 - should overwrite oldest (0)
    rb.push_overwrite(99);

    // Should get 1,2,3,99 in that order
    assert_test!(
        rb.try_pop() == Some(1),
        "overwrite test failed (expected 1)"
    );
    pass!()
}

/// Test ring buffer wrap around
pub fn test_ring_buffer_wrap() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 4> = RingBuffer::new();
    rb.try_push(1);
    rb.try_push(2);
    rb.try_push(3);

    rb.try_pop();
    rb.try_pop();

    rb.try_push(4);
    rb.try_push(5);
    rb.try_push(6);

    assert_test!(rb.try_pop() == Some(3), "wrap expected 3");
    assert_test!(rb.try_pop() == Some(4), "wrap expected 4");
    assert_test!(rb.try_pop() == Some(5), "wrap expected 5");
    assert_test!(rb.try_pop() == Some(6), "wrap expected 6");
    pass!()
}

/// Test ring buffer reset
pub fn test_ring_buffer_reset() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let mut rb: RingBuffer<u32, 8> = RingBuffer::new();
    rb.try_push(1);
    rb.try_push(2);
    rb.try_push(3);

    rb.reset();

    assert_test!(rb.is_empty(), "buffer should be empty after reset");
    assert_test!(rb.len() == 0, "length should be 0 after reset");
    pass!()
}

/// Test ring buffer capacity
pub fn test_ring_buffer_capacity() -> TestResult {
    use slopos_lib::ring_buffer::RingBuffer;

    let rb: RingBuffer<u32, 16> = RingBuffer::new();
    assert_test!(rb.capacity() == 16, "capacity should be 16");
    pass!()
}

// ============================================================================
// IRQMUTEX TESTS - 3 tests
// ============================================================================

/// Test 1: IrqMutex basic lock/unlock with guard
pub fn test_irqmutex_basic() -> TestResult {
    use slopos_lib::IrqMutex;

    let mutex: IrqMutex<u32> = IrqMutex::new(42);

    {
        let guard = mutex.lock();
        assert_test!(*guard == 42, "IrqMutex value should be 42");
    }

    {
        let guard = mutex.lock();
        assert_test!(*guard == 42, "IrqMutex value should still be 42");
    }

    pass!()
}

/// Test 2: IrqMutex mutation through guard
pub fn test_irqmutex_mutation() -> TestResult {
    use slopos_lib::IrqMutex;

    let mutex: IrqMutex<u32> = IrqMutex::new(0);

    {
        let mut guard = mutex.lock();
        *guard = 100;
    }

    {
        let guard = mutex.lock();
        if *guard != 100 {
            return fail!("IrqMutex mutation failed, got {}", *guard);
        }
    }

    pass!()
}

/// Test 3: IrqMutex try_lock
pub fn test_irqmutex_try_lock() -> TestResult {
    use slopos_lib::IrqMutex;

    let mutex: IrqMutex<u32> = IrqMutex::new(55);

    {
        let maybe_guard = mutex.try_lock();
        assert_test!(
            maybe_guard.is_some(),
            "try_lock on unlocked mutex should succeed"
        );
        let guard = maybe_guard.unwrap();
        assert_test!(*guard == 55, "try_lock value should be 55");
    }

    pass!()
}

// ============================================================================
// SHARED MEMORY TESTS - 8 tests
// ============================================================================

use crate::shared_memory::{
    shm_create, shm_destroy, shm_get_buffer_info, shm_get_ref_count, surface_attach,
};

/// Test 1: Create and destroy shared memory buffer
pub fn test_shm_create_destroy() -> TestResult {
    let owner = 1u32;
    let size = 4096u64;

    let token = shm_create(owner, size, 0);
    assert_test!(token != 0, "shm_create failed");

    let (phys, buf_size, buf_owner) = shm_get_buffer_info(token);
    if phys.is_null() {
        shm_destroy(owner, token);
        return fail!("buffer info phys is null");
    }
    if buf_size < size as usize {
        shm_destroy(owner, token);
        return fail!("buffer size {} < requested {}", buf_size, size);
    }
    if buf_owner != owner {
        shm_destroy(owner, token);
        return fail!("owner mismatch {} != {}", buf_owner, owner);
    }

    assert_test!(shm_destroy(owner, token) == 0, "shm_destroy failed");
    pass!()
}

/// Test 2: Create with zero size should fail
pub fn test_shm_create_zero_size() -> TestResult {
    let token = shm_create(1, 0, 0);
    if token != 0 {
        shm_destroy(1, token);
        return fail!("shm_create with zero size should fail");
    }
    pass!()
}

/// Test 3: Create with excessive size should fail
pub fn test_shm_create_excessive_size() -> TestResult {
    let token = shm_create(1, 128 * 1024 * 1024, 0);
    if token != 0 {
        shm_destroy(1, token);
        return fail!("shm_create with excessive size should fail");
    }
    pass!()
}

/// Test 4: Destroy by non-owner should fail
pub fn test_shm_destroy_non_owner() -> TestResult {
    let owner = 1u32;
    let non_owner = 2u32;

    let token = shm_create(owner, 4096, 0);
    assert_test!(token != 0, "shm_create failed");

    if shm_destroy(non_owner, token) == 0 {
        shm_destroy(owner, token);
        return fail!("non-owner destroy should fail");
    }

    shm_destroy(owner, token);
    pass!()
}

/// Test 5: Reference counting
pub fn test_shm_refcount() -> TestResult {
    let owner = 1u32;

    let token = shm_create(owner, 4096, 0);
    assert_test!(token != 0, "shm_create failed");

    let ref_count = shm_get_ref_count(token);
    if ref_count != 1 {
        shm_destroy(owner, token);
        return fail!("initial refcount should be 1, got {}", ref_count);
    }

    shm_destroy(owner, token);
    pass!()
}

/// Test 6: Get info for invalid token
pub fn test_shm_invalid_token() -> TestResult {
    let invalid_token = 99999u32;

    let (phys, size, owner) = shm_get_buffer_info(invalid_token);
    assert_test!(
        phys.is_null() && size == 0 && owner == 0,
        "invalid token should return null info"
    );
    pass!()
}

/// Test 7: Surface attach
pub fn test_shm_surface_attach() -> TestResult {
    let owner = 1u32;
    let width = 640u32;
    let height = 480u32;
    let size = (width as u64) * (height as u64) * 4;

    let token = shm_create(owner, size, 0);
    assert_test!(token != 0, "shm_create failed");

    if surface_attach(owner, token, width, height) != 0 {
        shm_destroy(owner, token);
        return fail!("surface_attach failed");
    }

    shm_destroy(owner, token);
    pass!()
}

/// Test 8: Surface attach with insufficient buffer
pub fn test_shm_surface_attach_too_small() -> TestResult {
    let owner = 1u32;

    let token = shm_create(owner, 4096, 0);
    assert_test!(token != 0, "shm_create failed");

    // 1920x1080x4 = 8,294,400 bytes - should fail
    if surface_attach(owner, token, 1920, 1080) == 0 {
        shm_destroy(owner, token);
        return fail!("surface_attach with too small buffer should fail");
    }

    shm_destroy(owner, token);
    pass!()
}

/// Test 9: Map shared buffer more than MAX_MAPPINGS_PER_BUFFER times
/// BUG FINDER: shm_map uses unwrap() on mapping slot search - will panic!
pub fn test_shm_mapping_overflow() -> TestResult {
    use crate::shared_memory::{ShmAccess, shm_map};

    let owner = 1u32;
    let size = 4096u64;

    let token = shm_create(owner, size, 0);
    assert_test!(token != 0, "shm_create failed");

    // MAX_MAPPINGS_PER_BUFFER is 8 - try to map 10 times using different process IDs
    let mut mapped_count = 0u32;
    for process_id in 1..=10u32 {
        let vaddr = shm_map(process_id, token, ShmAccess::ReadOnly);
        if vaddr != 0 {
            mapped_count += 1;
        }
    }

    shm_destroy(owner, token);

    if mapped_count > 8 {
        return fail!("BUG - mapped {} times, max should be 8", mapped_count);
    }
    pass!()
}

pub fn test_shm_surface_attach_overflow() -> TestResult {
    let owner = 1u32;
    let token = shm_create(owner, 64 * 1024 * 1024, 0);
    if token == 0 {
        return fail!("create large buffer for overflow test");
    }

    let result = surface_attach(owner, token, 0xFFFF, 0xFFFF);
    if result == 0 {
        shm_destroy(owner, token);
        return fail!("BUG - surface_attach accepted 0xFFFF x 0xFFFF (potential overflow)");
    }

    let result2 = surface_attach(owner, token, 0x8000_0000, 2);
    if result2 == 0 {
        shm_destroy(owner, token);
        return fail!("BUG - surface_attach accepted 0x80000000 x 2 (32-bit overflow)");
    }

    shm_destroy(owner, token);
    pass!()
}

// ============================================================================
// RIGOROUS MEMORY TESTS - Actually verify memory contents
// ============================================================================

pub fn test_page_alloc_write_verify() -> TestResult {
    let phys = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "allocate page");

    let virt = match phys.to_virt_checked() {
        Some(v) => v,
        None => {
            free_page_frame(phys);
            return fail!("get virtual address");
        }
    };

    let ptr = virt.as_mut_ptr::<u8>();

    // Write 0xAA/0x55 alternating pattern
    for i in 0..4096 {
        unsafe {
            let val = if i % 2 == 0 { 0xAA } else { 0x55 };
            ptr.add(i).write_volatile(val);
        }
    }

    // Read back and verify
    for i in 0..4096 {
        let expected = if i % 2 == 0 { 0xAA } else { 0x55 };
        let actual = unsafe { ptr.add(i).read_volatile() };
        if actual != expected {
            free_page_frame(phys);
            return fail!(
                "memory corruption at offset {}: expected {:#x}, got {:#x}",
                i,
                expected,
                actual
            );
        }
    }

    free_page_frame(phys);
    pass!()
}

pub fn test_page_alloc_zero_full_page() -> TestResult {
    let phys = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "allocate zeroed page");

    let virt = match phys.to_virt_checked() {
        Some(v) => v,
        None => {
            free_page_frame(phys);
            return fail!("get virtual address");
        }
    };

    let ptr = virt.as_mut_ptr::<u8>();

    for i in 0..4096 {
        let val = unsafe { ptr.add(i).read_volatile() };
        if val != 0 {
            free_page_frame(phys);
            return fail!("zeroed page has non-zero at offset {}: {:#x}", i, val);
        }
    }

    free_page_frame(phys);
    pass!()
}

pub fn test_page_alloc_no_stale_data() -> TestResult {
    let phys1 = alloc_page_frame(0);
    assert_not_null!(phys1.as_u64() as *const u8, "first alloc");

    if let Some(virt) = phys1.to_virt_checked() {
        let ptr = virt.as_mut_ptr::<u8>();
        for i in 0..4096 {
            unsafe { ptr.add(i).write_volatile(0xDE) };
        }
    }

    free_page_frame(phys1);

    // Allocate with ZERO flag - should be zeroed even if same page reused
    let phys2 = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys2.as_u64() as *const u8, "second alloc with zero flag");

    if let Some(virt) = phys2.to_virt_checked() {
        let ptr = virt.as_mut_ptr::<u8>();
        for i in 0..256 {
            let val = unsafe { ptr.add(i).read_volatile() };
            if val != 0 {
                free_page_frame(phys2);
                return fail!("stale data found at offset {}: {:#x} (expected 0)", i, val);
            }
        }
    }

    free_page_frame(phys2);
    pass!()
}

/// Test: Heap allocation boundary - verify we can use full allocated size
pub fn test_heap_boundary_write() -> TestResult {
    let sizes = [16usize, 32, 64, 128, 256, 512, 1024];

    for &size in &sizes {
        let ptr = kmalloc(size);
        if ptr.is_null() {
            return fail!("allocate {} bytes", size);
        }

        let byte_ptr = ptr as *mut u8;

        for i in 0..size {
            unsafe { byte_ptr.add(i).write_volatile((i & 0xFF) as u8) };
        }

        for i in 0..size {
            let expected = (i & 0xFF) as u8;
            let actual = unsafe { byte_ptr.add(i).read_volatile() };
            if actual != expected {
                kfree(ptr);
                return fail!(
                    "heap corruption at size={} offset={}: expected {:#x}, got {:#x}",
                    size,
                    i,
                    expected,
                    actual
                );
            }
        }

        kfree(ptr);
    }

    pass!()
}

/// Test: Multiple allocations don't overlap
pub fn test_heap_no_overlap() -> TestResult {
    const NUM_ALLOCS: usize = 8;
    let mut ptrs: [*mut c_void; NUM_ALLOCS] = [ptr::null_mut(); NUM_ALLOCS];
    let sizes = [64usize, 128, 256, 64, 512, 128, 256, 64];

    for i in 0..NUM_ALLOCS {
        ptrs[i] = kmalloc(sizes[i]);
        if ptrs[i].is_null() {
            for j in 0..i {
                kfree(ptrs[j]);
            }
            return fail!("allocate block {}", i);
        }

        let byte_ptr = ptrs[i] as *mut u8;
        for j in 0..sizes[i] {
            unsafe { byte_ptr.add(j).write_volatile(i as u8) };
        }
    }

    // Verify all allocations still have their patterns (no overlap)
    for i in 0..NUM_ALLOCS {
        let byte_ptr = ptrs[i] as *mut u8;
        for j in 0..sizes[i] {
            let actual = unsafe { byte_ptr.add(j).read_volatile() };
            if actual != i as u8 {
                for k in 0..NUM_ALLOCS {
                    kfree(ptrs[k]);
                }
                return fail!(
                    "allocation {} corrupted at offset {}: expected {:#x}, got {:#x}",
                    i,
                    j,
                    i as u8,
                    actual
                );
            }
        }
    }

    for i in 0..NUM_ALLOCS {
        kfree(ptrs[i]);
    }
    pass!()
}

/// Test: Double-free doesn't crash (defensive)
pub fn test_heap_double_free_defensive() -> TestResult {
    let ptr = kmalloc(64);
    assert_not_null!(ptr, "alloc 64 bytes");

    kfree(ptr);
    // Second free - should not crash (may be a no-op or error)
    kfree(ptr);
    pass!()
}

/// Test: Allocate large block, verify entire region is writable
pub fn test_heap_large_block_integrity() -> TestResult {
    let size = 8192usize;
    let ptr = kmalloc(size);
    assert_not_null!(ptr, "allocate 8KB");

    let byte_ptr = ptr as *mut u8;

    for i in 0..size {
        let pattern = ((i * 17) & 0xFF) as u8;
        unsafe { byte_ptr.add(i).write_volatile(pattern) };
    }

    for i in 0..size {
        let expected = ((i * 17) & 0xFF) as u8;
        let actual = unsafe { byte_ptr.add(i).read_volatile() };
        if actual != expected {
            kfree(ptr);
            return fail!(
                "large block corruption at offset {}: expected {:#x}, got {:#x}",
                i,
                expected,
                actual
            );
        }
    }

    kfree(ptr);
    pass!()
}

/// Test: Stress test - rapid alloc/free cycles
pub fn test_heap_stress_cycles() -> TestResult {
    for cycle in 0..100 {
        let ptr = kmalloc(128);
        if ptr.is_null() {
            return fail!("stress test failed at cycle {}", cycle);
        }

        let byte_ptr = ptr as *mut u8;
        unsafe {
            byte_ptr.write_volatile(0xAB);
            byte_ptr.add(127).write_volatile(0xCD);
        }

        let first = unsafe { byte_ptr.read_volatile() };
        let last = unsafe { byte_ptr.add(127).read_volatile() };

        if first != 0xAB || last != 0xCD {
            kfree(ptr);
            return fail!(
                "stress corruption at cycle {}: first={:#x}, last={:#x}",
                cycle,
                first,
                last
            );
        }

        kfree(ptr);
    }

    pass!()
}

pub fn test_page_alloc_multipage_integrity() -> TestResult {
    let phys = alloc_page_frames(4, ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "allocate 4 pages");

    for page in 0..4u64 {
        let page_phys = PhysAddr::new(phys.as_u64() + page * 4096);
        if let Some(virt) = page_phys.to_virt_checked() {
            let ptr = virt.as_mut_ptr::<u8>();
            for i in 0..4096 {
                let pattern = ((page as u8).wrapping_mul(17)).wrapping_add((i & 0xFF) as u8);
                unsafe { ptr.add(i).write_volatile(pattern) };
            }
        }
    }

    for page in 0..4u64 {
        let page_phys = PhysAddr::new(phys.as_u64() + page * 4096);
        if let Some(virt) = page_phys.to_virt_checked() {
            let ptr = virt.as_mut_ptr::<u8>();
            for i in 0..4096 {
                let expected = ((page as u8).wrapping_mul(17)).wrapping_add((i & 0xFF) as u8);
                let actual = unsafe { ptr.add(i).read_volatile() };
                if actual != expected {
                    free_page_frame(phys);
                    return fail!(
                        "multipage corruption page={} offset={}: expected {:#x}, got {:#x}",
                        page,
                        i,
                        expected,
                        actual
                    );
                }
            }
        }
    }

    free_page_frame(phys);
    pass!()
}

// ============================================================================
// PROCESS VM AND COW TESTS - Test the dangerous stuff
// ============================================================================

use crate::cow::{handle_cow_fault, is_cow_fault};
use crate::paging::{map_page_4kb_in_dir, paging_mark_cow, virt_to_phys_in_dir};
use crate::paging_defs::PageFlags;
use crate::test_fixtures::ProcessVmGuard;

pub fn test_process_vm_create_destroy_memory() -> TestResult {
    let Some(vm) = ProcessVmGuard::new() else {
        return fail!("create VM");
    };

    // The process should have a stack mapped - try to access it via kernel
    let null_page_phys = virt_to_phys_in_dir(vm.page_dir, VirtAddr::new(0));
    if null_page_phys.is_null() {
        klog_info!("PROCESS_TEST: Null page not mapped (expected for user process)");
    }

    pass!()
}

pub fn test_process_vm_alloc_and_access() -> TestResult {
    let Some(vm) = ProcessVmGuard::new() else {
        return fail!("create VM");
    };

    use crate::process_vm::process_vm_alloc;
    let user_addr = process_vm_alloc(vm.pid, 4096, PageFlags::WRITABLE.bits() as u32);
    assert_test!(user_addr != 0, "process_vm_alloc returned 0");

    // The allocation is LAZY - pages aren't mapped until accessed
    let phys = virt_to_phys_in_dir(vm.page_dir, VirtAddr::new(user_addr));
    if !phys.is_null() {
        if let Some(virt) = phys.to_virt_checked() {
            let ptr = virt.as_mut_ptr::<u8>();
            unsafe {
                ptr.write_volatile(0x42);
                let val = ptr.read_volatile();
                assert_test!(val == 0x42, "memory write/read mismatch");
            }
        }
    }

    pass!()
}

pub fn test_process_vm_brk_expansion() -> TestResult {
    let Some(vm) = ProcessVmGuard::new() else {
        return fail!("create VM");
    };

    use crate::process_vm::process_vm_brk;

    let initial_brk = process_vm_brk(vm.pid, 0);
    assert_test!(initial_brk != 0, "initial brk is 0");

    let new_brk = process_vm_brk(vm.pid, initial_brk + 8192);
    if new_brk <= initial_brk {
        return fail!("brk expansion failed: {} -> {}", initial_brk, new_brk);
    }

    let shrunk_brk = process_vm_brk(vm.pid, initial_brk + 4096);
    if shrunk_brk != initial_brk + 4096 {
        return fail!(
            "brk shrink failed: expected {}, got {}",
            initial_brk + 4096,
            shrunk_brk
        );
    }

    pass!()
}

pub fn test_cow_page_isolation() -> TestResult {
    let Some(parent) = ProcessVmGuard::new() else {
        return fail!("create parent VM");
    };

    // Use process_vm_alloc to properly create a VMA (COW clone iterates VMAs, not raw mappings)
    use crate::process_vm::process_vm_alloc;
    let test_addr = process_vm_alloc(parent.pid, PAGE_SIZE_4KB, PageFlags::WRITABLE.bits() as u32);
    assert_test!(test_addr != 0, "process_vm_alloc failed");

    // Allocate physical page and map it within the VMA
    let phys = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "alloc page frame");

    if map_page_4kb_in_dir(
        parent.page_dir,
        VirtAddr::new(test_addr),
        phys,
        PageFlags::USER_RW.bits(),
    ) != 0
    {
        free_page_frame(phys);
        return fail!("map page in parent");
    }

    // Write pattern via HHDM
    if let Some(virt) = phys.to_virt_checked() {
        let ptr = virt.as_mut_ptr::<u8>();
        for i in 0..4096 {
            unsafe { ptr.add(i).write_volatile(0xAA) };
        }
    }

    // Clone with COW
    let Some(child) = parent.clone_cow() else {
        return fail!("COW clone");
    };

    // Both should point to the same physical page initially (COW sharing)
    let parent_phys = virt_to_phys_in_dir(parent.page_dir, VirtAddr::new(test_addr));
    let child_phys = virt_to_phys_in_dir(child.page_dir, VirtAddr::new(test_addr));

    if parent_phys.is_null() || child_phys.is_null() {
        return fail!(
            "COW pages not mapped correctly (parent={:?}, child={:?})",
            parent_phys,
            child_phys
        );
    }

    if parent_phys != child_phys {
        klog_info!("PROCESS_TEST: COW pages should share same physical page initially");
    }

    // Verify child can read the same data
    if let Some(virt) = child_phys.to_virt_checked() {
        let ptr = virt.as_mut_ptr::<u8>();
        let val = unsafe { ptr.read_volatile() };
        if val != 0xAA {
            return fail!("child COW page has wrong data: {:#x}", val);
        }
    }

    pass!()
}

pub fn test_cow_fault_handling() -> TestResult {
    let Some(vm) = ProcessVmGuard::new() else {
        return fail!("create VM");
    };

    let test_addr = 0x2000u64;
    let phys = alloc_page_frame(ALLOC_FLAG_ZERO);
    assert_not_null!(phys.as_u64() as *const u8, "alloc page frame");

    if map_page_4kb_in_dir(
        vm.page_dir,
        VirtAddr::new(test_addr),
        phys,
        PageFlags::USER_RO.bits(),
    ) != 0
    {
        free_page_frame(phys);
        return fail!("map page as RO");
    }

    // Mark as COW
    paging_mark_cow(vm.page_dir, VirtAddr::new(test_addr));

    // Simulate a write fault - error code for write to present page = 0x03
    let error_code = 0x03u64;
    let is_cow = is_cow_fault(error_code, vm.page_dir, test_addr);
    assert_test!(is_cow, "is_cow_fault returned false for COW page");

    // Handle the COW fault
    match handle_cow_fault(vm.page_dir, test_addr) {
        Ok(()) => {}
        Err(e) => {
            return fail!("handle_cow_fault failed: {:?}", e);
        }
    }

    // After COW resolution, page should be writable
    let new_phys = virt_to_phys_in_dir(vm.page_dir, VirtAddr::new(test_addr));
    assert_test!(!new_phys.is_null(), "page unmapped after COW resolution");

    if let Some(virt) = new_phys.to_virt_checked() {
        let ptr = virt.as_mut_ptr::<u8>();
        unsafe {
            ptr.write_volatile(0xBB);
            let val = ptr.read_volatile();
            assert_test!(val == 0xBB, "post-COW write verification failed");
        }
    }

    pass!()
}

pub fn test_multiple_process_vms() -> TestResult {
    const NUM_PROCESSES: usize = 5;
    let mut pids = [0u32; NUM_PROCESSES];

    init_process_vm();

    for i in 0..NUM_PROCESSES {
        pids[i] = create_process_vm();
        if pids[i] == INVALID_PROCESS_ID {
            for j in 0..i {
                destroy_process_vm(pids[j]);
            }
            return fail!("create process {}", i);
        }
    }

    // Verify each has a unique page directory
    let mut dirs = [ptr::null_mut(); NUM_PROCESSES];
    for i in 0..NUM_PROCESSES {
        dirs[i] = process_vm_get_page_dir(pids[i]);
        if dirs[i].is_null() {
            for j in 0..NUM_PROCESSES {
                destroy_process_vm(pids[j]);
            }
            return fail!("process {} has null page dir", i);
        }
    }

    // Check uniqueness
    for i in 0..NUM_PROCESSES {
        for j in (i + 1)..NUM_PROCESSES {
            if dirs[i] == dirs[j] {
                for k in 0..NUM_PROCESSES {
                    destroy_process_vm(pids[k]);
                }
                return fail!("processes {} and {} share same page dir!", i, j);
            }
        }
    }

    for i in 0..NUM_PROCESSES {
        destroy_process_vm(pids[i]);
    }
    pass!()
}

pub fn test_vma_flags_retrieval() -> TestResult {
    let Some(vm) = ProcessVmGuard::new() else {
        return fail!("create VM");
    };

    use crate::process_vm::{process_vm_alloc, process_vm_get_vma_flags};
    use crate::vma_flags::VmaFlags;

    let user_addr = process_vm_alloc(vm.pid, 8192, PageFlags::WRITABLE.bits() as u32);
    assert_test!(user_addr != 0, "process_vm_alloc returned 0");

    let flags = process_vm_get_vma_flags(vm.pid, user_addr);
    assert_test!(flags.is_some(), "VMA flags not found for allocated region");

    let flags = flags.unwrap();
    assert_test!(
        flags.contains(VmaFlags::HEAP),
        "allocated region not marked as HEAP"
    );
    assert_test!(
        flags.contains(VmaFlags::WRITE),
        "allocated region not marked as WRITE"
    );

    pass!()
}

// ============================================================================
// PAT (PAGE ATTRIBUTE TABLE) TESTS
// ============================================================================

pub fn test_pat_wc_enabled() -> TestResult {
    const MEM_TYPE_WC: u8 = 0x01;

    let pat_msr = cpu::read_msr(Msr::PAT);
    let pat1 = ((pat_msr >> 8) & 0xFF) as u8;

    if pat1 != MEM_TYPE_WC {
        klog_info!(
            "PAT_TEST: PAT[1] is {:#x} (expected WC={:#x}) - framebuffer will be slow!",
            pat1,
            MEM_TYPE_WC
        );
        klog_info!("PAT_TEST: Full PAT MSR = {:#018x}", pat_msr);
        return fail!("PAT[1] is {:#x} (expected WC={:#x})", pat1, MEM_TYPE_WC);
    }

    pass!()
}

// ============================================================================
// SUITE REGISTRATION — tests are auto-collected via linker section
// ============================================================================

use slopos_lib::define_test_suite;

define_test_suite!(
    vm,
    [test_process_vm_slot_reuse, test_process_vm_counter_reset,]
);

define_test_suite!(
    heap,
    [
        test_heap_free_list_search,
        test_heap_fragmentation_behind_head,
    ]
);

define_test_suite!(
    page_alloc,
    [
        test_page_alloc_single,
        test_page_alloc_multi_order,
        test_page_alloc_free_cycle,
        test_page_alloc_zeroed,
        test_page_alloc_refcount,
        test_page_alloc_stats,
        test_page_alloc_free_null,
        test_page_alloc_fragmentation,
    ]
);

define_test_suite!(
    heap_ext,
    [
        test_heap_warmup_pages_minimum,
        test_heap_small_alloc,
        test_heap_medium_alloc,
        test_heap_large_alloc,
        test_heap_kzalloc_zeroed,
        test_heap_kfree_null,
        test_heap_alloc_zero,
        test_heap_stats,
        test_global_alloc_vec,
    ]
);

define_test_suite!(
    paging,
    [
        test_paging_virt_to_phys,
        test_paging_get_kernel_dir,
        test_paging_user_accessible_kernel,
        test_paging_cow_kernel,
        test_pat_wc_enabled,
    ]
);

define_test_suite!(
    ring_buf,
    [
        test_ring_buffer_basic,
        test_ring_buffer_fifo,
        test_ring_buffer_empty_pop,
        test_ring_buffer_full,
        test_ring_buffer_overwrite,
        test_ring_buffer_wrap,
        test_ring_buffer_reset,
        test_ring_buffer_capacity,
    ]
);

define_test_suite!(
    irqmutex,
    [
        test_irqmutex_basic,
        test_irqmutex_mutation,
        test_irqmutex_try_lock,
    ]
);

define_test_suite!(
    shm,
    [
        test_shm_create_destroy,
        test_shm_create_zero_size,
        test_shm_create_excessive_size,
        test_shm_destroy_non_owner,
        test_shm_refcount,
        test_shm_invalid_token,
        test_shm_surface_attach,
        test_shm_surface_attach_too_small,
        test_shm_surface_attach_overflow,
        test_shm_mapping_overflow,
    ]
);

define_test_suite!(
    rigorous,
    [
        test_page_alloc_write_verify,
        test_page_alloc_zero_full_page,
        test_page_alloc_no_stale_data,
        test_heap_boundary_write,
        test_heap_no_overlap,
        test_heap_double_free_defensive,
        test_heap_large_block_integrity,
        test_heap_stress_cycles,
        test_page_alloc_multipage_integrity,
    ]
);

define_test_suite!(
    process_vm,
    [
        test_process_vm_create_destroy_memory,
        test_process_vm_alloc_and_access,
        test_process_vm_brk_expansion,
        test_cow_page_isolation,
        test_cow_fault_handling,
        test_multiple_process_vms,
        test_vma_flags_retrieval,
    ]
);
